{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## ISPR 23/24, third midterm\n",
    "### by Andrea Marino (matr. 561935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#from PIL import Image              # Python Image Library\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(colab := 'google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"drive/MyDrive/ISPR/Compitini ISPR/Terzo Compitino/\"\n",
    "    sys.path.insert(0,BASE_PATH)\n",
    "    #os.chdir(BASE_PATH)\n",
    "    #!pip install -U torch           # update PyTorch\n",
    "    #!pip install -U torchvision     # update torchvision\n",
    "    !pip install -U optuna           # update optuna\n",
    "else:\n",
    "    BASE_PATH = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "#import optuna\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set PyTorch's seed to a fixed value, for reproducibility but also to get the same splits in the datasets we're going to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7e697dd48b90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I already used the dataset for the first assignment, I don't need to download it again. I just have to fetch it from the right directory (in a way that deals with both Colab and my local install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Primo Compitino/weizmann_horse_db/horse/',\n",
       " '../Primo Compitino/weizmann_horse_db/mask/')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HORSE_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/horse/\")\n",
    "MASK_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/mask/\")\n",
    "\n",
    "HORSE_PATH, MASK_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create the class for the dataset, as required by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply transformations on our data. The principal transformation worth talking about has to deal with the batch size and the shape of the images. \n",
    "\n",
    "If in our `DataLoader`s the batch size is set to be greater than 1, then the images have to be of the same shape (height * width), otherwise the associated tensors can't be stacked in a single batch tensor. This is unfortunate, we can deal with it in three ways:\n",
    "1. Always online learning algorithm (aka batch_size=1)\n",
    "2. Resize the image\n",
    "3. Add a padding of zeros to the smaller images, to bring them all to the same size as the bigger image in the dataset (and then let the model deal with the extra padding).\n",
    "\n",
    "All three options are interesting for their own reasons, so writing a more general code that allows one to apply each of the ideas is worthwhile.\n",
    "\n",
    "The first option allows to assess the model's performance on images of different sizes, an interesting challenge for sure. The second option is the most promising and most powerful one, as torchvision's tools seems to do a good job at resizing both the images and the mask, upon preliminary checking. The third option is pretty rough, but it might be interesting to see how the model performs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "class HorseDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 image_path=HORSE_PATH,\n",
    "                 mask_path=MASK_PATH,\n",
    "                 transform=None, # normalize pixel values\n",
    "                 target_transform=None,):\n",
    "        self.img_dir = image_path\n",
    "        self.mask_dir = mask_path\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # We know that the answer is 327 but let's make it\n",
    "        # more general and structured\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self._horsePath(idx+1)\n",
    "        img = read_image(img_path)      \n",
    "        mask = read_image(mask_path)  \n",
    "        img  = img.float()          # convert to float\n",
    "        mask = mask.long()          # convert to long\n",
    "        # transforms given as input\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        # standard transforms\n",
    "        img = img/torch.max(img)    # normalize pixel values\n",
    "        #mask[mask>0] = 1.0          # set all non-zero values to 1\n",
    "        return img, mask\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self.__getitem__(i)    \n",
    "    \n",
    "    def _horsePath(self, h:int):\n",
    "        '''\n",
    "        Returns the path to the horse image\n",
    "        whose number (in the filename) is h\n",
    "        '''\n",
    "        number = \"0\"*(2-int(np.log10(h)))+str(h)\n",
    "        imgname = \"horse\" + number + \".png\"\n",
    "        img_path = self.img_dir + imgname\n",
    "        mask_path = self.mask_dir + imgname\n",
    "        return img_path, mask_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the transformations as described, we may need the height and width of the biggest and smallest images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest image is horse 65, of size 960x1280\n",
      "The smallest image is horse 16, of size 148x231\n"
     ]
    }
   ],
   "source": [
    "plain_dataset = HorseDataset()\n",
    "\n",
    "MAX, MIN = (0,0), (np.inf,np.inf)\n",
    "biggest, smallest = -1,-1\n",
    "\n",
    "for i, (img, _) in enumerate(plain_dataset,start=1):\n",
    "    img_h, img_w = tuple(img.shape[1:])\n",
    "    if img_h*img_w > MAX[0]*MAX[1]:\n",
    "        MAX = (img_h, img_w)\n",
    "        biggest = i\n",
    "    if img_h*img_w < MIN[0]*MIN[1]:\n",
    "        MIN = (img_h, img_w)\n",
    "        smallest = i\n",
    "\n",
    "print(f\"The biggest image is horse {biggest}, of size {MAX[0]}x{MAX[1]}\")\n",
    "print(f\"The smallest image is horse {smallest}, of size {MIN[0]}x{MIN[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute a random permutation that will be used consistently across the three datasets, to split them randomly, but in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Compute the sizes of the training and test sets\n",
    "train_size = int(0.8 * len(plain_dataset))\n",
    "test_size = len(plain_dataset) - train_size\n",
    "\n",
    "# Compute the split indices\n",
    "indices = torch.randperm(len(plain_dataset)).tolist()\n",
    "train_indices = indices[:train_size]\n",
    "test_indices = indices[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of unchanged images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset following the first option.\n",
    "\n",
    "We can use the `random_split` to create the training set and the test set. \n",
    "\n",
    "20% of the whole dataset is held out as test set. I chose this approach over cross-validation to alleviate the computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pre-computed indices to split the dataset\n",
    "training_plain_dataset = Subset(plain_dataset, train_indices)\n",
    "test_plain_dataset = Subset(plain_dataset, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLoader will be created later, when they'll be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of resized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the `HorseDataset` class allows us to create a dataset of resized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "\n",
    "#resize_transform = lambda h,w: Lambda(lambda X: Resize((h,w))(X))\n",
    "def resize_transform(height:int, width:int):\n",
    "    return Lambda(lambda X: Resize((height,width))(X))\n",
    "\n",
    "# We can resize to the smallest, or choose any other size\n",
    "h, w = 360, 480\n",
    "\n",
    "dataset_resized = HorseDataset(transform=resize_transform(h,w), \n",
    "                              target_transform=resize_transform(h,w))\n",
    "\n",
    "train_resized_dataset = Subset(dataset_resized, train_indices)\n",
    "test_resized_dataset = Subset(dataset_resized, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of padded images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a dataset of padded images using the `HorseDataset` class.\n",
    "\n",
    "Instead of passing a fixed amount of padding to the transformation, we pass it the target height and width of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Pad\n",
    "\n",
    "#pad_transform = lambda target_h, target_w: Lambda(lambda X: Pad((X.shape[1]-target_h, X.shape[2]-target_w), fill=0)(X))\n",
    "def pad_transform(target_h:int, target_w:int):\n",
    "    return Lambda(lambda X: Pad((target_h-X.shape[1], target_w-X.shape[2]), fill=0)(X))\n",
    "\n",
    "h, w = MAX\n",
    "\n",
    "dataset_padded = HorseDataset(transform=pad_transform(h,w), \n",
    "                              target_transform=pad_transform(h,w))\n",
    "\n",
    "train_padded_dataset = Subset(dataset_padded, train_indices)\n",
    "test_padded_dataset = Subset(dataset_padded, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is semantic segmentation of the horse from the background. The chosen model for this task is a *U-shaped network* that uses convolution and transposed convolutions. Given the context, such a network could be referred as a horseshoe network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our horseshoe network has two parts: an encoder and a decoder. For the sake of simplicity (and cleanness of code), these are split into two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture I chose is heavily inspired by [SegNet](https://arxiv.org/abs/1511.00561), because it's a network that was designed with the precise purpose of image segmentation, in an efficient way (parameter-wise).\n",
    "\n",
    "The architecture I propose is modular enough to incorporate some changes, though. For example, a feed-forward layer between the encoder and the decoder can be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the `HorseshoeEncoder`. Even though that's somewhat ambiguous and deplorable (the ambiguity is also in the literature), inside a convolutional layer three things will happen:\n",
    "- A given number of convolutional filters will be applied\n",
    "- The (batch of) resulting feature maps will be normalized\n",
    "- An activation function is applied\n",
    "\n",
    "The `conv_layer` nested function returns a convolutional layer as a `Sequential` container. \n",
    "\n",
    "We can specify the structure of our decoder via the `architecture` parameter of the constructor. This parameter is a list of tuple, each tuple being relative to a *block*. \n",
    "\n",
    "In this context, a block is just a sequence of convolutional filters followed by a max pooling. More precisely, each tuple in the list specifies how many convolutional layers we want to stack before applying the pooling (1st component of the tuple), and how many convolutional filters there are in each layer inside the block (2nd component of the tuple).\n",
    "\n",
    "So, in abstract terms, our encoder is just a sequence of blocks. In practice, each block is again a Sequential module, and the encoder's layers are implemented as a `nn.ModuleDict`. A dictionary of modules is used because we need to keep track of the pooling indices of the maximum element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],\n",
    "                 in_channels:int=3,) -> None:\n",
    "        super(HorseshoeEncoder, self).__init__()\n",
    "\n",
    "        def conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        in_channels = in_channels  # initial number of input channels\n",
    "        for i, (n_blocks, out_channels) in enumerate(architecture):\n",
    "            conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                conv_layers.append(conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                in_channels = out_channels\n",
    "            conv_block = nn.Sequential(*conv_layers)\n",
    "            # We add a MaxPooling layer after the prescribed number of conv layers\n",
    "            self.layers.add_module(f'conv_block_{i}',nn.ModuleList([conv_block, nn.MaxPool2d(2,2,return_indices=True)]))\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooling_indices = []\n",
    "        input_sizes = []\n",
    "        #for block in self.layers.values():\n",
    "        for i in range(len(self.layers)):\n",
    "            conv_block, pooling_layer = self.layers[f'conv_block_{i}']\n",
    "            # Apply the convolutional layer\n",
    "            x = conv_block(x)\n",
    "            # we need to keep track of the input size\n",
    "            # before the pooling, to do the unpooling correctly at decoding time\n",
    "            input_sizes.append(x.size())\n",
    "            # Apply the pooling layer\n",
    "            x, indices = pooling_layer(x)\n",
    "            # We need to keep track of the indices for the unpooling\n",
    "            pooling_indices.append(indices)\n",
    "        x = self.conv1x1(x)\n",
    "\n",
    "        return x, pooling_indices, input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is the mirror image of the encoder. We pass to it the same architecture parameter that was passed to the encoder, and the network is built from right to left in a kind of unintuitive fashion that allows to keep the simmetry and clarity of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],\n",
    "                 out_channels:int=2,) -> None:\n",
    "        super(HorseshoeDecoder, self).__init__()\n",
    "\n",
    "        def transp_conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        out_channels = out_channels     # desired nÂ° of output channels\n",
    "        arch_len = len(architecture)\n",
    "        for i, (n_blocks, in_channels) in enumerate(architecture):\n",
    "            transp_conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                transp_conv_layers.insert(0,transp_conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                out_channels = in_channels\n",
    "            transp_conv_block = nn.Sequential(*transp_conv_layers)\n",
    "            # Since we're building the architecture from right to left, we need to add \n",
    "            # (more precisely, index) the modules in reversed order in the dictionary\n",
    "            self.layers.add_module(f'transp_conv_block_{arch_len-i-1}',nn.ModuleList([nn.MaxUnpool2d(2,2), transp_conv_block])) \n",
    "\n",
    "        self.Tconv1x1 = nn.ConvTranspose2d(256, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, pooling_indices, input_sizes):\n",
    "        x = self.Tconv1x1(x)\n",
    "        #for block in reversed(self.layers.values()):\n",
    "        for i in range(len(self.layers)):\n",
    "            unpooling_layer, transp_conv_block = self.layers[f'transp_conv_block_{i}']\n",
    "            # First the unpooling (which needs the indices used for pooling and the size of the input)...\n",
    "            x = unpooling_layer(x, pooling_indices.pop(), output_size=input_sizes.pop())\n",
    "            # ... then the transposed convolution!\n",
    "            x = transp_conv_block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full `HorseshoeNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've separately written the encoder and the decoder, to create our horseshoe network we just have to put them together, basically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels:int=3,\n",
    "                 out_channels:int=2,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],):\n",
    "        super(HorseshoeNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.encoder = HorseshoeEncoder(architecture=architecture,in_channels=in_channels)\n",
    "\n",
    "        # Transpose conv. layers\n",
    "        self.decoder = HorseshoeDecoder(architecture=architecture,out_channels=out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x, pooling_indices, input_sizes = self.encoder(x)\n",
    "\n",
    "        # Decoding\n",
    "        x = self.decoder(x, pooling_indices, input_sizes)\n",
    "\n",
    "        # Final softmax (classification) (not needed for CrossEntropyLoss)\n",
    "        #x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the transposed convolutional blocks are listed in reversed order in the decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for training the model: auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model and the dataset, it's time to write the training cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the typical PyTorch's paradygm, let's define a function that performs one cycle of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer) -> None:\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        assert torch.unique(y).tolist() == [0., 1.], f\"Error: targets should be in [0, 1] for BCELoss. Instead got: {torch.unique(y).tolist()}, type: {y.dtype}\"\n",
    "        assert X.shape[1] == 3, f\"Error: input images should have 3 channels. Instead got: {X.shape[1]}\"\n",
    "        assert torch.max(X) <= 1 and torch.min(X) >= 0, f\"Error: input images should be normalized in [0, 1]. Instead got: min {torch.min(X).item()} and max {torch.max(X).item()}\"\n",
    "\n",
    "            ## Forward pass:\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "\n",
    "        # Compute loss\n",
    "        # We get rid of the channel dimension in the mask, to compute the loss (it's 1)\n",
    "        loss = loss_fn(pred, y.squeeze(1))\n",
    "\n",
    "            ## Backpropagation:\n",
    "        # Zero out the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the gradient (backward step)\n",
    "        loss.backward()\n",
    "        # apply weight update\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to evaluate the model (aka to test it). That's what the next function does. \n",
    "\n",
    "This function can return multiple metrics, but for this assignment only the accuracy will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(y_true, \n",
    "                  y_pred, \n",
    "                  score_fn:str='accuracy') -> float:   \n",
    "    # turn the probabilities into labels\n",
    "    # (i.e. get the index of the tensor in which the probability is the highest)\n",
    "    y_pred_bin = y_pred.argmax(dim=1).float()\n",
    "    \n",
    "    y_true = y_true.squeeze(1)\n",
    "\n",
    "    TPs = (y_true * y_pred_bin).sum().item()\n",
    "    TNs = ((1 - y_true) * (1 - y_pred_bin)).sum().item()\n",
    "    FPs = ((1 - y_true) * y_pred_bin).sum().item()\n",
    "    FNs = (y_true * (1 - y_pred_bin)).sum().item()\n",
    "\n",
    "    # Since I compute the average, tuurns out the formulas are the ones\n",
    "    # below\n",
    "    score = {}\n",
    "    score['accuracy'] = (TPs + TNs) / (TPs + TNs + FPs + FNs)\n",
    "    score['precision'] = TPs / (TPs + FPs) if TPs + FPs != 0 else 0\n",
    "    score['recall'] = TPs / (TPs + FNs) if TPs + FNs != 0 else 0\n",
    "\n",
    "    if score_fn == 'whole':\n",
    "        return score\n",
    "    else:\n",
    "        return score[score_fn.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, loss_fn) -> float:\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    avg_accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "\n",
    "            # compute the loss, and accumulate it\n",
    "            test_loss += loss_fn(pred, y.squeeze(1)).item()\n",
    "\n",
    "            # compute accuracy, and accumulate it\n",
    "            avg_accuracy += compute_score(y_true=y, y_pred=pred, score_fn='accuracy')\n",
    "    # Average over the number of batches\n",
    "    avg_accuracy /= num_batches\n",
    "    test_loss /= num_batches\n",
    "    return test_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the auxiliary functions, we're ready to write the training cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaderd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining which dataset we want to use, among those created with the three strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the analysis in this notebook, and the grid search, only the dataset with resized images was used. Individual, localized trials have been performed and the `HorseshoeNetwork` can also handle the non-resized images.*\n",
    "\n",
    "*Since the non-resized images aren't batched, training on them is significantly slower, since parallelism can't be exploited (this has been verified in exploratory trials). This suggests that the non-resizing approach - though interesting - isn't really viable outside of a small scale.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = train_resized_dataset\n",
    "test_dataset = test_resized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can further split the developement set to hold out the validation set. I choose this approach over K-fold cross validation (which would have been more suitable, given the limited size of the dataset) just because of constraints in terms of computational resources and time.\n",
    "\n",
    "To recap, 20% of our data is test set, 16% is validation set (20% of 80%) and the remaining 64% is training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally create the `DataLoaders` that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "train_size = int(0.8 * len(dev_dataset))\n",
    "val_size = len(dev_dataset) - train_size\n",
    "\n",
    "training_dataset, validation_dataset = random_split(dev_dataset, [train_size, val_size])\n",
    "\n",
    "# Watch out! The non-reshaped images must use batch-size 1\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for grid searching. Unfortunately, the hyperparameter's list is not very complete, but upon individual trials it was found that the values:\n",
    "- architecture=`[(2,4),(3,8),(3,16),(3,16)]`\n",
    "- learning rate = `1e-3`\n",
    "- weight decay = `1e-5`\n",
    "yielded good results in as few as 10 epochs (above 70% accuracy in validation).\n",
    "\n",
    "I would have liked to explore also with a little padding, and with dilated convolutions, but - again - due to constraints in terms of time and computational resources I had to restrict the scope of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the listed architectures are not very big. Neither they're very deep, but the task is binary classification, and the objects shouldn't be too hard to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid\n",
    "lr_list = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "weight_decay_list = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "architectures_list = [\n",
    "    [(2,16),(3,32),(3,64)],\n",
    "    [(2,4),(3,8),(3,16),(4,16)], \n",
    "    [(2,8),(3,16),(3,32),(3,32)], \n",
    "    [(2,16),(3,32),(3,64),(3,64)],\n",
    "    #[(2,32),(3,64),(3,128),(3,128)],\n",
    "    [(2,16),(2,32),(3,64),(3,128),(3,128)]\n",
    "]\n",
    "\n",
    "'''\n",
    "lr_list_mk2 = [1e-2, 1e-3, 1e-4]\n",
    "weight_decay_list_mk2 = [1e-4, 1e-5, 1e-6]\n",
    "architectures_list_mk2 = [\n",
    "    # small:\n",
    "    [(2,16),(3,32),(3,64)],\n",
    "    # medium:\n",
    "    [(3,8),(3,16),(3,32),(4,64)],\n",
    "    [(2,16),(3,32),(3,64),(3,64)],\n",
    "    [(2,16),(3,32),(3,64),(3,128)],\n",
    "    # big\n",
    "    [(2,16),(2,32),(3,64),(3,128),(3,128)]\n",
    "    [(2,32),(2,64),(3,64),(3,128),(3,128)]\n",
    "]'''\n",
    "\n",
    "# patience for early stopping\n",
    "patience = 5\n",
    "\n",
    "results_df = pd.DataFrame(columns=['lr', 'weight_decay', 'architecture', 'train_loss_history', 'val_loss_history', 'train_accuracy_history', 'val_accuracy_history', 'epochs', 'best_val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search 1 of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "Training completed after 8 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.587795, avg accuracy: 74.6511% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.564432, avg accuracy: 75.2357% \n",
      "\n",
      "Search 2 of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose2d(\n",
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "Training completed after 25 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.355867, avg accuracy: 84.6813% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.341494, avg accuracy: 86.1307% \n",
      "\n",
      "Search 3 of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose2d(\n",
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "Training completed after 11 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.414220, avg accuracy: 79.8806% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.400400, avg accuracy: 80.7555% \n",
      "\n",
      "Search 4 of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "Training completed after 17 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.514692, avg accuracy: 77.7639% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.492176, avg accuracy: 78.9859% \n",
      "\n",
      "Search 5 of 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "Training completed after 17 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.620712, avg accuracy: 59.7941% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.635281, avg accuracy: 58.9802% \n",
      "\n",
      "Search 6 of 80\n",
      "Early stopping...\n",
      "Training completed after 22 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.436341, avg accuracy: 79.4563% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.378953, avg accuracy: 80.6269% \n",
      "\n",
      "Search 7 of 80\n",
      "Early stopping...\n",
      "Training completed after 18 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.454053, avg accuracy: 81.4896% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.423664, avg accuracy: 82.7433% \n",
      "\n",
      "Search 8 of 80\n",
      "Early stopping...\n",
      "Training completed after 41 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.299866, avg accuracy: 87.2650% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.312466, avg accuracy: 86.8833% \n",
      "\n",
      "Search 9 of 80\n",
      "Early stopping...\n",
      "Training completed after 19 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.371330, avg accuracy: 83.1372% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.350959, avg accuracy: 84.3760% \n",
      "\n",
      "Search 10 of 80\n",
      "Early stopping...\n",
      "Training completed after 31 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.299169, avg accuracy: 87.4971% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.307860, avg accuracy: 87.0971% \n",
      "\n",
      "Search 11 of 80\n",
      "Early stopping...\n",
      "Training completed after 22 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.340286, avg accuracy: 84.7889% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.336411, avg accuracy: 84.9628% \n",
      "\n",
      "Search 12 of 80\n",
      "Early stopping...\n",
      "Training completed after 19 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.316748, avg accuracy: 87.2023% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.316631, avg accuracy: 87.5718% \n",
      "\n",
      "Search 13 of 80\n",
      "Early stopping...\n",
      "Training completed after 21 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.360136, avg accuracy: 74.7020% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.348169, avg accuracy: 75.6049% \n",
      "\n",
      "Search 14 of 80\n",
      "Early stopping...\n",
      "Training completed after 29 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.270064, avg accuracy: 88.0934% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.269836, avg accuracy: 88.2112% \n",
      "\n",
      "Search 15 of 80\n",
      "Early stopping...\n",
      "Training completed after 16 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.298197, avg accuracy: 87.7184% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.294039, avg accuracy: 87.7791% \n",
      "\n",
      "Search 16 of 80\n",
      "Early stopping...\n",
      "Training completed after 24 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.388518, avg accuracy: 79.9963% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.378338, avg accuracy: 80.6785% \n",
      "\n",
      "Search 17 of 80\n",
      "Early stopping...\n",
      "Training completed after 43 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.194374, avg accuracy: 92.4365% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.190903, avg accuracy: 92.9437% \n",
      "\n",
      "Search 18 of 80\n",
      "Early stopping...\n",
      "Training completed after 18 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.317272, avg accuracy: 85.1418% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.306696, avg accuracy: 85.7691% \n",
      "\n",
      "Search 19 of 80\n",
      "Early stopping...\n",
      "Training completed after 38 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.161650, avg accuracy: 93.3157% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.187493, avg accuracy: 92.4805% \n",
      "\n",
      "Search 20 of 80\n",
      "Early stopping...\n",
      "Training completed after 39 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.190906, avg accuracy: 92.2378% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.220839, avg accuracy: 91.1096% \n",
      "\n",
      "Search 21 of 80\n",
      "Early stopping...\n",
      "Training completed after 50 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.262794, avg accuracy: 90.7485% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.269929, avg accuracy: 90.4241% \n",
      "\n",
      "Search 22 of 80\n",
      "Training completed after 50 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.188916, avg accuracy: 95.0182% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.209151, avg accuracy: 93.9918% \n",
      "\n",
      "Search 23 of 80\n",
      "Early stopping...\n",
      "Training completed after 50 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.245366, avg accuracy: 92.6618% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.271673, avg accuracy: 91.6817% \n",
      "\n",
      "Search 24 of 80\n",
      "Early stopping...\n",
      "Training completed after 33 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.274719, avg accuracy: 90.3873% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.277732, avg accuracy: 90.5715% \n",
      "\n",
      "Search 25 of 80\n",
      "Early stopping...\n",
      "Training completed after 13 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.400658, avg accuracy: 88.0074% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.404837, avg accuracy: 87.7508% \n",
      "\n",
      "Search 26 of 80\n",
      "Early stopping...\n",
      "Training completed after 19 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.638154, avg accuracy: 75.3957% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.598680, avg accuracy: 77.9238% \n",
      "\n",
      "Search 27 of 80\n",
      "Training completed after 50 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.177376, avg accuracy: 95.7953% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.210869, avg accuracy: 94.1094% \n",
      "\n",
      "Search 28 of 80\n",
      "Early stopping...\n",
      "Training completed after 48 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.185596, avg accuracy: 95.3319% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.217488, avg accuracy: 93.5738% \n",
      "\n",
      "Search 29 of 80\n",
      "Early stopping...\n",
      "Training completed after 37 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.222134, avg accuracy: 95.1038% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.250526, avg accuracy: 93.7583% \n",
      "\n",
      "Search 30 of 80\n",
      "Early stopping...\n",
      "Training completed after 42 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.207093, avg accuracy: 95.3118% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.216895, avg accuracy: 94.8065% \n",
      "\n",
      "Search 31 of 80\n",
      "Training completed after 50 epochs\n",
      "Train Error: \n",
      " \tAvg loss: 0.212956, avg accuracy: 94.0241% \n",
      "\n",
      "Validation Error: \n",
      " \tAvg loss: 0.235995, avg accuracy: 92.8835% \n",
      "\n",
      "Search 32 of 80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m epochs_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m     22\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(test_dataloader, model, loss_fn)\n\u001b[1;32m     23\u001b[0m train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m evaluate(train_dataloader, model, loss_fn)\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(dataloader, model, loss_fn, optimizer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      4\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39munique(y)\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: targets should be in [0, 1] for BCELoss. Instead got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39munique(y)\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/utils/data/dataset.py:417\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T_co]:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 417\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mHorseDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     22\u001b[0m     img_path, mask_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_horsePath(idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     img \u001b[38;5;241m=\u001b[39m read_image(img_path)      \n\u001b[1;32m     24\u001b[0m     mask \u001b[38;5;241m=\u001b[39m read_image(mask_path)  \n\u001b[1;32m     25\u001b[0m     img  \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mfloat()          \u001b[38;5;66;03m# convert to float\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torchvision/io/image.py:276\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    274\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m    275\u001b[0m data \u001b[38;5;241m=\u001b[39m read_file(path)\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode, apply_exif_orientation\u001b[38;5;241m=\u001b[39mapply_exif_orientation)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torchvision/io/image.py:249\u001b[0m, in \u001b[0;36mdecode_image\u001b[0;34m(input, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    248\u001b[0m     _log_api_usage_once(decode_image)\n\u001b[0;32m--> 249\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_image(\u001b[38;5;28minput\u001b[39m, mode\u001b[38;5;241m.\u001b[39mvalue, apply_exif_orientation)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/_ops.py:854\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;66;03m# named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "MAX_EPOCHS = 50 # I really don't want to go farther than this...\n",
    "EARLY_STOPPING = True\n",
    "\n",
    "for i, (lr, wd, arch) in enumerate(product(lr_list, weight_decay_list, architectures_list), start=1):\n",
    "    print(f\"Search {i} of {len(lr_list)*len(weight_decay_list)*len(architectures_list)}\")\n",
    "    torch.cuda.empty_cache()    # let's try\n",
    "    model = HorseshoeNetwork(architecture=arch)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    \n",
    "    epochs_count = 0\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    val_loss_list, val_accuracy_list = [], []\n",
    "    train_loss_list, train_accuracy_list = [], []\n",
    "    for _ in range(MAX_EPOCHS):\n",
    "        epochs_count += 1\n",
    "        # Training\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "        val_loss, val_accuracy = evaluate(test_dataloader, model, loss_fn)\n",
    "        train_loss, train_accuracy = evaluate(train_dataloader, model, loss_fn)\n",
    "\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_accuracy_list.append(val_accuracy)\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "        if EARLY_STOPPING:\n",
    "            # Check if validation loss has improved\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_count = 0\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "\n",
    "            # Check if early stopping criterion is met\n",
    "            if early_stop_count >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "\n",
    "    print(f\"Training completed after {epochs_count} epochs\")\n",
    "    print(f\"Train Error: \\n \\tAvg loss: {train_loss:>8f}, avg accuracy: {100*train_accuracy:>2.4f}% \\n\")\n",
    "    print(f\"Validation Error: \\n \\tAvg loss: {val_loss:>8f}, avg accuracy: {100*val_accuracy:>2.4f}% \\n\")\n",
    "    results_df.loc[len(results_df.index)] = [lr, wd, arch, train_loss_list, val_loss_list, train_accuracy_list, val_accuracy_list, epochs_count, best_val_loss]\n",
    "\n",
    "print(\"Grid search completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the results\n",
    "results_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the grid search is completed, we can retrieve the best hyperparameters, and retrain the model to see the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the row with the lowest validation loss\n",
    "min_val_loss_index = results_df['val_loss'].idxmin()\n",
    "\n",
    "# Get the row with the lowest validation loss\n",
    "best_row_dict = results_df.loc[min_val_loss_index].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a model with the best hyperparameters we found, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HorseshoeNetwork(architecture=best_row_dict['architecture'])\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_row_dict['lr'], weight_decay=best_row_dict['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, train_accuracy_list = [], []\n",
    "val_loss_list, val_accuracy_list = [], []\n",
    "\n",
    "# dunno about this. I should aim at the validation loss\n",
    "EPOCHS = best_row_dict['epochs']\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    # Compute the loss and accuracy\n",
    "    tr_loss, tr_acc = evaluate(train_dataloader, model, loss_fn)\n",
    "    val_loss, val_acc = evaluate(val_dataloader, model, loss_fn)\n",
    "\n",
    "    # print\n",
    "    print(f\"Train Error: \\n \\tAvg loss: {tr_loss:>8f}, avg accuracy: {100*tr_acc:>2.4f}% \\n\")\n",
    "    print(f\"Validation Error: \\n \\tAvg loss: {val_loss:>8f}, avg accuracy: {100*val_acc:>2.4f}% \\n\")\n",
    "    # Store loss and accuracy for the current epoch\n",
    "    train_loss_list.append(tr_loss)\n",
    "    train_accuracy_list.append(tr_acc)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_acc)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our training curves, which we can display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].plot(train_loss_list, label='Training loss', color='blue')\n",
    "ax[0].plot(val_loss_list, label='Validation loss', color='red')\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_accuracy_list, label='Training Accuracy')\n",
    "ax[1].plot(val_accuracy_list, label='Validation Accuracy')\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've selected the model and assessed the validation error. We can retrain on the whole developement set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try on a test image, just to visualize a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = test_resized_dataset[0]\n",
    "img, mask = img.to(device), mask.to(device)\n",
    "pred = model(img[None, ...])    # add a batch dimension\n",
    "\n",
    "\n",
    "pred_bin = pred.argmax(1).float()       # binarize the prediction\n",
    "pred_bin = pred_bin.squeeze()           # remove the batch dimension and the channel dimension\n",
    "\n",
    "print(pred.shape, mask.shape, img.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "ax[0].imshow(img.cpu().permute(1, 2, 0))\n",
    "ax[0].set_title('Image', fontsize=16, fontweight='bold')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(mask.cpu().squeeze(), cmap='gray')\n",
    "ax[1].set_title('Mask', fontsize=16, fontweight='bold')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(pred_bin.cpu().detach(), cmap='gray')\n",
    "ax[2].set_title('Prediction', fontsize=16, fontweight='bold')\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how to do semantic segmentation with an encoder-decoder architecture based on convolutional network, using the `HorseshoeNetwork` proposed in this notebook.\n",
    "\n",
    "We've seen that the architecture of the encoder is based on *blocks* of *convolutional layers* followed by max pooling layers. The architecture of the decoder mirrors that of the encoder. All in all, the network is versatile, as it can be easily and clairly be defined by a single parameter, and could be extended by adding a feedforward layer between encoder and decoder. \n",
    "\n",
    "The `HorseshoeNetwork` could also deal with multi-class classification (i.e. semantic segmentation), and can deal with images of different dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the scope of the exploration isn't as broad as what I would have liked it to be. Noticeably, these interesting aspects were left out:\n",
    "- K-fold cross validation for model selection\n",
    "- Hyperparameter optimization using Optuna (I wrote the code and ran it, hoping to save some time w.r.t. full grid search, but it turned out not to be so...)\n",
    "- Select among models that used different strategies for building the dataset (padding/resizing/leaving them be), and discuss their performance to tackle the question \"what's the best way to feed data to a convolutional network?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Ettore the dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISPR-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
