{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## ISPR 23/24, third assignment\n",
    "### by Andrea Marino (matr. 561935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import sys\n",
    "#from PIL import Image              # Python Image Library\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(colab := 'google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"drive/MyDrive/ISPR/Compitini ISPR/Terzo Compitino/\"\n",
    "    sys.path.insert(0,BASE_PATH)\n",
    "    #os.chdir(BASE_PATH)\n",
    "    #!pip install -U torch           # update PyTorch\n",
    "    #!pip install -U torchvision     # update torchvision\n",
    "else:\n",
    "    BASE_PATH = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set PyTorch's seed to a fixed value, for reproducibility but also to get the same splits in the datasets we're going to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79caf1528a30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data, Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I already used the dataset for the first assignment, I don't need to download it again. I just have to fetch it from the right directory (in a way that deals with both Colab and my local install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Primo Compitino/weizmann_horse_db/horse/',\n",
       " '../Primo Compitino/weizmann_horse_db/mask/')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HORSE_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/horse/\")\n",
    "MASK_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/mask/\")\n",
    "\n",
    "HORSE_PATH, MASK_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create the class for the dataset, as required by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply transformations on our data. The principal transformation worth talking about has to deal with the batch size and the shape of the images. \n",
    "\n",
    "If in our `DataLoader`s the batch size is set to be greater than 1, then the images have to be of the same shape (height * width), otherwise the associated tensors can't be stacked in a single batch tensor. This is unfortunate, we can deal with it in three ways:\n",
    "1. Always online learning algorithm (aka batch_size=1)\n",
    "2. Resize the image\n",
    "3. Add a padding of zeros to the smaller images, to bring them all to the same size as the bigger image in the dataset (and then let the model deal with the extra padding).\n",
    "\n",
    "All three options are interesting for their own reasons, so writing a more general code that allows one to apply each of the ideas is worthwhile.\n",
    "\n",
    "The first option allows to assess the model's performance on images of different sizes, an interesting challenge for sure. The second option is the most promising and most powerful one, as torchvision's tools seems to do a good job at resizing both the images and the mask, upon preliminary checking. The third option is pretty rough, but it might be interesting to see how the model performs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "class HorseDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 image_path=HORSE_PATH,\n",
    "                 mask_path=MASK_PATH,\n",
    "                 transform=Lambda(lambda img: img/255), # normalize pixel values\n",
    "                 target_transform=None):\n",
    "        self.img_dir = image_path\n",
    "        self.mask_dir = mask_path\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # We know that the answer is 327 but let's make it\n",
    "        # more general and structured\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self._horsePath(idx+1)\n",
    "        img = read_image(img_path)      \n",
    "        mask = read_image(mask_path)   \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        return img, mask\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self.__getitem__(i)    \n",
    "    \n",
    "    def _horsePath(self, h:int):\n",
    "        '''\n",
    "        Returns the path to the horse image\n",
    "        whose number (in the filename) is h\n",
    "        '''\n",
    "        number = \"0\"*(2-int(np.log10(h)))+str(h)\n",
    "        imgname = \"horse\" + number + \".png\"\n",
    "        img_path = self.img_dir + imgname\n",
    "        mask_path = self.mask_dir + imgname\n",
    "        return img_path, mask_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the transformations as described, we need the height and width of the biggest and smallest images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest image is horse 65, of size 960x1280\n",
      "The smallest image is horse 16, of size 148x231\n"
     ]
    }
   ],
   "source": [
    "plain_dataset = HorseDataset()\n",
    "\n",
    "MAX, MIN = (0,0), (np.inf,np.inf)\n",
    "biggest, smallest = -1,-1\n",
    "\n",
    "for i, (img, _) in enumerate(plain_dataset,start=1):\n",
    "    img_h, img_w = tuple(img.shape[1:])\n",
    "    if img_h*img_w > MAX[0]*MAX[1]:\n",
    "        MAX = (img_h, img_w)\n",
    "        biggest = i\n",
    "    if img_h*img_w < MIN[0]*MIN[1]:\n",
    "        MIN = (img_h, img_w)\n",
    "        smallest = i\n",
    "\n",
    "print(f\"The biggest image is horse {biggest}, of size {MAX[0]}x{MAX[1]}\")\n",
    "print(f\"The smallest image is horse {smallest}, of size {MIN[0]}x{MIN[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 590, 800]), mask shape: torch.Size([1, 590, 800])\n",
      "Image type: torch.float32, mask type: torch.uint8\n",
      "Image values range: 0.0 - 1.0\n",
      "Mask values range: 0 - 1\n"
     ]
    }
   ],
   "source": [
    "image0, mask0 = plain_dataset[0]\n",
    "print(f\"Image shape: {image0.shape}, mask shape: {mask0.shape}\")\n",
    "print(f\"Image type: {image0.dtype}, mask type: {mask0.dtype}\")\n",
    "print(f\"Image values range: {torch.min(image0).item()} - {torch.max(image0).item()}\")\n",
    "print(f\"Mask values range: {torch.min(mask0).item()} - {torch.max(mask0).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of unchanged images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset following the first option.\n",
    "\n",
    "We can use the `random_split` to create the training set and the test set. \n",
    "\n",
    "20% of the whole dataset is held out as test set. I chose this approach over cross-validation to alleviate the computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create the developement and test datasets\n",
    "train_size = int(0.8 * len(plain_dataset))\n",
    "test_size = len(plain_dataset) - train_size\n",
    "\n",
    "training_plain_dataset, test_plain_dataset = random_split(plain_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the DataLoaders for the training and test data, using batch size of 1 since we applied no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Attenzione:\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_plain_dataloader = DataLoader(training_plain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_plain_dataloader = DataLoader(test_plain_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of resized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the `HorseDataset` class allows us to create a dataset of resized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "\n",
    "#resize_transform = lambda h,w: Lambda(lambda X: Resize((h,w))(X))\n",
    "def resize_transform(height:int, width:int):\n",
    "    return Lambda(lambda X: Resize((height,width))(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images and masks have been resized correctly\n"
     ]
    }
   ],
   "source": [
    "# We can resize to the smallest, or choose any other size\n",
    "h, w = 256, 256\n",
    "\n",
    "dataset_resized = HorseDataset(transform=resize_transform(h,w), \n",
    "                              target_transform=resize_transform(h,w))\n",
    "\n",
    "for img, mask in dataset_resized:\n",
    "    assert img.shape == (3,h,w) and mask.shape == (1,h,w)\n",
    "\n",
    "print(\"All images and masks have been resized correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_resized_dataset, test_resized_dataset = random_split(dataset_resized, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_resized_dataloader = DataLoader(training_resized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_resized_dataloader = DataLoader(test_resized_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: n° images is 261, n° of minibatches is: 66\n",
      "Test: n° images is 66, n° of minibatches is: 66\n",
      "Shape of X [N, C, H, W]:  torch.Size([1, 3, 580, 504])\n",
      "Shape of y: [N, C, H, W] torch.Size([1, 1, 580, 504]) torch.uint8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training: n° images is {len(train_plain_dataloader.dataset)}, n° of minibatches is: {len(test_plain_dataloader)}\")\n",
    "print(f\"Test: n° images is {len(test_plain_dataloader.dataset)}, n° of minibatches is: {len(test_plain_dataloader)}\")\n",
    "\n",
    "for X, y in train_plain_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: [N, C, H, W]\", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of padded images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a dataset of padded images using the `HorseDataset` class.\n",
    "\n",
    "Instead of passing a fixed amount of padding to the transformation, we pass it the target height and width of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Pad\n",
    "\n",
    "#pad_transform = lambda target_h, target_w: Lambda(lambda X: Pad((X.shape[1]-target_h, X.shape[2]-target_w), fill=0)(X))\n",
    "def pad_transform(target_h:int, target_w:int):\n",
    "    return Lambda(lambda X: Pad((X.shape[1]-target_h, X.shape[2]-target_w), fill=0)(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = MAX\n",
    "\n",
    "dataset_padded = HorseDataset(transform=pad_transform(h,w), \n",
    "                              target_transform=pad_transform(h,w))\n",
    "\n",
    "# Check that things are ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded_dataset, test_padded_dataset = random_split(dataset_padded, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_padded_dataloader = DataLoader(training_padded_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_padded_dataloader = DataLoader(test_padded_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is semantic segmentation of the horse from the background. The chosen model for this task is a *U-shaped network* that uses convolution and transposed convolutions. Given the context, such a network could be referred as a horseshoe network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our horseshoe network has two parts: an encoder and a decoder. For the sake of simplicity (and cleanness of code), these are split into two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the `HorseshoeEncoder`. Even though that's somewhat ambiguous and deplorable (the ambiguity is also in the literature), inside a convolutional layer three things will happen:\n",
    "- A given number of convolutional filters will be applied\n",
    "- The (batch of) resulting feature maps will be normalized\n",
    "- An activation function is applied\n",
    "\n",
    "The `conv_layer` nested function returns a convolutional layer as a `Sequential` container. \n",
    "\n",
    "We can specify the structure of our decoder via the `architecture` parameter of the constructor. This parameter is a list of tuple, each tuple being relative to a *block*. A block in this context is just a sequence of convolutional filters followed by a max pooling. More precisely, each tuple in the list specifies how many convolutional layers we want to stack before applying the pooling (1st component of the tuple), and how many convolutional filters there are in each layer inside the block.\n",
    "\n",
    "In abstract terms, our encoder is just a sequence of blocks. In practice, each block is again a Sequential module, and the encoder's layers are implemented as a `nn.ModuleDict`. A dictionary of modules is used because we need to keep track of the pooling indices of the maximum element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],) -> None:\n",
    "        super(HorseshoeEncoder, self).__init__()\n",
    "\n",
    "        def conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        in_channels = 3  # initial number of input channels\n",
    "        for i, (n_blocks, out_channels) in enumerate(architecture):\n",
    "            conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                conv_layers.append(conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                in_channels = out_channels\n",
    "            conv_block = nn.Sequential(*conv_layers)\n",
    "            # We add a MaxPooling layer after the prescribed number of conv layers\n",
    "            self.layers.add_module(f'conv_block_{i}',nn.ModuleList([conv_block, nn.MaxPool2d(2,2,return_indices=True)]))\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooling_indices = []\n",
    "        input_sizes = []\n",
    "        for block in self.layers.values():\n",
    "            conv_layer, pooling_layer = block\n",
    "            # Apply the convolutional layer\n",
    "            x = conv_layer(x)\n",
    "            # we need to keep track of the input size\n",
    "            # before the pooling, to do the unpooling correctly at decoding time\n",
    "            input_sizes.append(x.size())\n",
    "            # Apply the pooling layer\n",
    "            x, indices = pooling_layer(x)\n",
    "            # We need to keep track of the indices for the unpooling\n",
    "            pooling_indices.append(indices)\n",
    "        x = self.conv1x1(x)\n",
    "\n",
    "        return x, pooling_indices, input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(3,128),(3,128),(3,64),(2,32),(2,16)],) -> None:\n",
    "        super(HorseshoeDecoder, self).__init__()\n",
    "\n",
    "        def transp_conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        # WIP!!\n",
    "        _, in_channels = architecture[0]    # initial number of input channels\n",
    "\n",
    "        self.Tconv1x1 = nn.ConvTranspose2d(256, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        for i, (n_blocks, out_channels) in enumerate(architecture):\n",
    "            transp_conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                transp_conv_layers.append(transp_conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                in_channels = out_channels\n",
    "            transp_conv_block = nn.Sequential(*transp_conv_layers)\n",
    "            self.layers.add_module(f'transp_conv_block_{i}',nn.ModuleList([nn.MaxUnpool2d(2,2), transp_conv_block]))  \n",
    "\n",
    "    def forward(self, x, pooling_indices, input_sizes):\n",
    "        x = self.Tconv1x1(x)\n",
    "        for block in self.layers.values():\n",
    "            unpooling_layer, transp_conv_layer = block\n",
    "            # First the unpooling (which needs the indices used for pooling and the size of the input)...\n",
    "            x = unpooling_layer(x, pooling_indices.pop(), output_size=input_sizes.pop())\n",
    "            # ... then the transposed convolution!\n",
    "            x = transp_conv_layer(x)\n",
    "        # To obtain the final (a 1xHxW tensor) we use single a single\n",
    "        # 1x1 (transposed) convolutional filter\n",
    "        x = nn.ConvTranspose2d(x.size(1), 1, kernel_size=1, stride=1, padding=0)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full `HorseshoeNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've separately written the encoder and the decoder, to create our horseshoe network we just have to put them together, basically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeNetwork(nn.Module):\n",
    "\n",
    "    # Very much wip\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],):\n",
    "        super(HorseshoeNetwork, self).__init__()\n",
    "\n",
    "        # -------------- Convolutional layers --------------\n",
    "        self.encoder = HorseshoeEncoder(architecture)\n",
    "\n",
    "        # -------------- Transpose conv. layers --------------\n",
    "        self.decoder = HorseshoeDecoder(architecture[::-1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x, pooling_indices, input_sizes = self.encoder(x)\n",
    "\n",
    "        # Decoding\n",
    "        x = self.decoder(x, pooling_indices, input_sizes)\n",
    "\n",
    "        # Final sigmoid (classification)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 568, 800]) torch.Size([1, 1, 568, 800])\n",
      "torch.float32 torch.uint8\n"
     ]
    }
   ],
   "source": [
    "img, mask = next(iter(train_plain_dataloader))\n",
    "\n",
    "print(img.shape, mask.shape)\n",
    "print(img.dtype, mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = HorseshoeEncoder(architecture=[(1,16),(1,32)])\n",
    "encoder.to(device)\n",
    "\n",
    "dec_img, pooling_indices, input_sizes = encoder(img.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "Input size at level 0: torch.Size([1, 16, 566, 798])\n",
      "Pooling indices size at level 0: torch.Size([1, 16, 283, 399])\n",
      "\n",
      "Input size at level 1: torch.Size([1, 32, 281, 397])\n",
      "Pooling indices size at level 1: torch.Size([1, 32, 140, 198])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(input_sizes), len(pooling_indices))\n",
    "\n",
    "for i, (size, pool_idx) in enumerate(zip(input_sizes,pooling_indices)):\n",
    "    print(f\"Input size at level {i}: {size}\")\n",
    "    print(f\"Pooling indices size at level {i}: {pool_idx.shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected shape of indices to be same as that of the input tensor ([1, 32, 283, 399]) but got indices tensor with shape: [1, 16, 283, 399]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m decoder \u001b[38;5;241m=\u001b[39m HorseshoeDecoder(architecture\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m32\u001b[39m),(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m16\u001b[39m)])\n\u001b[0;32m----> 3\u001b[0m out \u001b[38;5;241m=\u001b[39m decoder(dec_img, pooling_indices, input_sizes)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 32\u001b[0m, in \u001b[0;36mHorseshoeDecoder.forward\u001b[0;34m(self, x, pooling_indices, input_sizes)\u001b[0m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTconv1x1(x)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# First the unpooling (which needs the indices used for pooling and the size of the input)...\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m block[\u001b[38;5;241m0\u001b[39m](x, pooling_indices\u001b[38;5;241m.\u001b[39mpop(), output_size\u001b[38;5;241m=\u001b[39minput_sizes\u001b[38;5;241m.\u001b[39mpop())\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# ... then the transposed convolution!\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m block[\u001b[38;5;241m1\u001b[39m](x)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/pooling.py:409\u001b[0m, in \u001b[0;36mMaxUnpool2d.forward\u001b[0;34m(self, input, indices, output_size)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, indices: Tensor, output_size: Optional[List[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmax_unpool2d(\u001b[38;5;28minput\u001b[39m, indices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    410\u001b[0m                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, output_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/functional.py:990\u001b[0m, in \u001b[0;36mmax_unpool2d\u001b[0;34m(input, indices, kernel_size, stride, padding, output_size)\u001b[0m\n\u001b[1;32m    988\u001b[0m padding \u001b[38;5;241m=\u001b[39m _pair(padding)\n\u001b[1;32m    989\u001b[0m output_size \u001b[38;5;241m=\u001b[39m _unpool_output_size(\u001b[38;5;28minput\u001b[39m, kernel_size, _stride, padding, output_size)\n\u001b[0;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmax_unpool2d(\u001b[38;5;28minput\u001b[39m, indices, output_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected shape of indices to be same as that of the input tensor ([1, 32, 283, 399]) but got indices tensor with shape: [1, 16, 283, 399]"
     ]
    }
   ],
   "source": [
    "decoder = HorseshoeDecoder(architecture=[(1,32),(1,16)])\n",
    "\n",
    "out = decoder(dec_img, pooling_indices, input_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prove strane e a caso. Esperimenti puntuali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISPR-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
