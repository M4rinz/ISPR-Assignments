{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## ISPR 23/24, third assignment\n",
    "### by Andrea Marino (matr. 561935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import sys\n",
    "#from PIL import Image              # Python Image Library\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(colab := 'google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"drive/MyDrive/ISPR/Compitini ISPR/Terzo Compitino/\"\n",
    "    sys.path.insert(0,BASE_PATH)\n",
    "    #os.chdir(BASE_PATH)\n",
    "    #!pip install -U torch           # update PyTorch\n",
    "    #!pip install -U torchvision     # update torchvision\n",
    "else:\n",
    "    BASE_PATH = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set PyTorch's seed to a fixed value, for reproducibility but also to get the same splits in the datasets we're going to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bcbc912ca30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data, Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I already used the dataset for the first assignment, I don't need to download it again. I just have to fetch it from the right directory (in a way that deals with both Colab and my local install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Primo Compitino/weizmann_horse_db/horse/',\n",
       " '../Primo Compitino/weizmann_horse_db/mask/')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HORSE_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/horse/\")\n",
    "MASK_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/mask/\")\n",
    "\n",
    "HORSE_PATH, MASK_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create the class for the dataset, as required by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply transformations on our data. The principal transformation worth talking about has to deal with the batch size and the shape of the images. \n",
    "\n",
    "If in our `DataLoader`s the batch size is set to be greater than 1, then the images have to be of the same shape (height * width), otherwise the associated tensors can't be stacked in a single batch tensor. This is unfortunate, we can deal with it in three ways:\n",
    "1. Always online learning algorithm (aka batch_size=1)\n",
    "2. Resize the image\n",
    "3. Add a padding of zeros to the smaller images, to bring them all to the same size as the bigger image in the dataset (and then let the model deal with the extra padding).\n",
    "\n",
    "All three options are interesting for their own reasons, so writing a more general code that allows one to apply each of the ideas is worthwhile.\n",
    "\n",
    "The first option allows to assess the model's performance on images of different sizes, an interesting challenge for sure. The second option is the most promising and most powerful one, as torchvision's tools seems to do a good job at resizing both the images and the mask, upon preliminary checking. The third option is pretty rough, but it might be interesting to see how the model performs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "class HorseDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 image_path=HORSE_PATH,\n",
    "                 mask_path=MASK_PATH,\n",
    "                 transform=Lambda(lambda img: img/255), # normalize pixel values\n",
    "                 target_transform=None):\n",
    "        self.img_dir = image_path\n",
    "        self.mask_dir = mask_path\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # We know that the answer is 327 but let's make it\n",
    "        # more general and structured\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self._horsePath(idx+1)\n",
    "        img = read_image(img_path)      \n",
    "        mask = read_image(mask_path)   \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        return img, mask\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self.__getitem__(i)    \n",
    "    \n",
    "    def _horsePath(self, h:int):\n",
    "        '''\n",
    "        Returns the path to the horse image\n",
    "        whose number (in the filename) is h\n",
    "        '''\n",
    "        number = \"0\"*(2-int(np.log10(h)))+str(h)\n",
    "        imgname = \"horse\" + number + \".png\"\n",
    "        img_path = self.img_dir + imgname\n",
    "        mask_path = self.mask_dir + imgname\n",
    "        return img_path, mask_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the transformations as described, we need the height and width of the biggest and smallest images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest image is horse 65, of size 960x1280\n",
      "The smallest image is horse 16, of size 148x231\n"
     ]
    }
   ],
   "source": [
    "plain_dataset = HorseDataset()\n",
    "\n",
    "MAX, MIN = (0,0), (np.inf,np.inf)\n",
    "biggest, smallest = -1,-1\n",
    "\n",
    "for i, (img, _) in enumerate(plain_dataset,start=1):\n",
    "    img_h, img_w = tuple(img.shape[1:])\n",
    "    if img_h*img_w > MAX[0]*MAX[1]:\n",
    "        MAX = (img_h, img_w)\n",
    "        biggest = i\n",
    "    if img_h*img_w < MIN[0]*MIN[1]:\n",
    "        MIN = (img_h, img_w)\n",
    "        smallest = i\n",
    "\n",
    "print(f\"The biggest image is horse {biggest}, of size {MAX[0]}x{MAX[1]}\")\n",
    "print(f\"The smallest image is horse {smallest}, of size {MIN[0]}x{MIN[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 590, 800]), mask shape: torch.Size([1, 590, 800])\n",
      "Image type: torch.float32, mask type: torch.uint8\n",
      "Image values range: 0.0 - 1.0\n",
      "Mask values range: 0 - 1\n"
     ]
    }
   ],
   "source": [
    "image0, mask0 = plain_dataset[0]\n",
    "print(f\"Image shape: {image0.shape}, mask shape: {mask0.shape}\")\n",
    "print(f\"Image type: {image0.dtype}, mask type: {mask0.dtype}\")\n",
    "print(f\"Image values range: {torch.min(image0).item()} - {torch.max(image0).item()}\")\n",
    "print(f\"Mask values range: {torch.min(mask0).item()} - {torch.max(mask0).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of unchanged images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset following the first option.\n",
    "\n",
    "We can use the `random_split` to create the training set and the test set. \n",
    "\n",
    "20% of the whole dataset is held out as test set. I chose this approach over cross-validation to alleviate the computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create the developement and test datasets\n",
    "train_size = int(0.8 * len(plain_dataset))\n",
    "test_size = len(plain_dataset) - train_size\n",
    "\n",
    "training_plain_dataset, test_plain_dataset = random_split(plain_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the DataLoaders for the training and test data, using batch size of 1 since we applied no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Attenzione:\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_plain_dataloader = DataLoader(training_plain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_plain_dataloader = DataLoader(test_plain_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of resized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the `HorseDataset` class allows us to create a dataset of resized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "\n",
    "#resize_transform = lambda h,w: Lambda(lambda X: Resize((h,w))(X))\n",
    "def resize_transform(height:int, width:int):\n",
    "    return Lambda(lambda X: Resize((height,width))(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images and masks have been resized correctly\n"
     ]
    }
   ],
   "source": [
    "# We can resize to the smallest, or choose any other size\n",
    "h, w = 256, 256\n",
    "\n",
    "dataset_resized = HorseDataset(transform=resize_transform(h,w), \n",
    "                              target_transform=resize_transform(h,w))\n",
    "\n",
    "for img, mask in dataset_resized:\n",
    "    assert img.shape == (3,h,w) and mask.shape == (1,h,w)\n",
    "\n",
    "print(\"All images and masks have been resized correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_resized_dataset, test_resized_dataset = random_split(dataset_resized, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_resized_dataloader = DataLoader(training_resized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_resized_dataloader = DataLoader(test_resized_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: n° images is 261, n° of minibatches is: 66\n",
      "Test: n° images is 66, n° of minibatches is: 66\n",
      "Shape of X [N, C, H, W]:  torch.Size([1, 3, 580, 504])\n",
      "Shape of y: [N, C, H, W] torch.Size([1, 1, 580, 504]) torch.uint8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training: n° images is {len(train_plain_dataloader.dataset)}, n° of minibatches is: {len(test_plain_dataloader)}\")\n",
    "print(f\"Test: n° images is {len(test_plain_dataloader.dataset)}, n° of minibatches is: {len(test_plain_dataloader)}\")\n",
    "\n",
    "for X, y in train_plain_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: [N, C, H, W]\", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of padded images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a dataset of padded images using the `HorseDataset` class.\n",
    "\n",
    "Instead of passing a fixed amount of padding to the transformation, we pass it the target height and width of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Pad\n",
    "\n",
    "#pad_transform = lambda target_h, target_w: Lambda(lambda X: Pad((X.shape[1]-target_h, X.shape[2]-target_w), fill=0)(X))\n",
    "def pad_transform(target_h:int, target_w:int):\n",
    "    return Lambda(lambda X: Pad((X.shape[1]-target_h, X.shape[2]-target_w), fill=0)(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = MAX\n",
    "\n",
    "dataset_padded = HorseDataset(transform=pad_transform(h,w), \n",
    "                              target_transform=pad_transform(h,w))\n",
    "\n",
    "# Check that things are ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded_dataset, test_padded_dataset = random_split(dataset_padded, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_padded_dataloader = DataLoader(training_padded_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_padded_dataloader = DataLoader(test_padded_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is semantic segmentation of the horse from the background. The chosen model for this task is a *U-shaped network* that uses convolution and transposed convolutions. Given the context, such a network could be referred as a horseshoe network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our horseshoe network has two parts: an encoder and a decoder. For the sake of simplicity (and cleanness of code), these are split into two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the `HorseshoeEncoder`. Even though that's somewhat ambiguous and deplorable (the ambiguity is also in the literature), inside a convolutional layer three things will happen:\n",
    "- A given number of convolutional filters will be applied\n",
    "- The (batch of) resulting feature maps will be normalized\n",
    "- An activation function is applied\n",
    "\n",
    "The `conv_layer` nested function returns a convolutional layer as a `Sequential` container. \n",
    "\n",
    "We can specify the structure of our decoder via the `architecture` parameter of the constructor. This parameter is a list of tuple, each tuple being relative to a *block*. A block in this context is just a sequence of convolutional filters followed by a max pooling. More precisely, each tuple in the list specifies how many convolutional layers we want to stack before applying the pooling (1st component of the tuple), and how many convolutional filters there are in each layer inside the block.\n",
    "\n",
    "In abstract terms, our encoder is just a sequence of blocks. In practice, each block is again a Sequential module, and the encoder's layers are implemented as a `nn.ModuleDict`. A dictionary of modules is used because we need to keep track of the pooling indices of the maximum element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],\n",
    "                 in_channels:int=3,) -> None:\n",
    "        super(HorseshoeEncoder, self).__init__()\n",
    "\n",
    "        def conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        in_channels = in_channels  # initial number of input channels\n",
    "        for i, (n_blocks, out_channels) in enumerate(architecture):\n",
    "            conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                conv_layers.append(conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                in_channels = out_channels\n",
    "            conv_block = nn.Sequential(*conv_layers)\n",
    "            # We add a MaxPooling layer after the prescribed number of conv layers\n",
    "            self.layers.add_module(f'conv_block_{i}',nn.ModuleList([conv_block, nn.MaxPool2d(2,2,return_indices=True)]))\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooling_indices = []\n",
    "        input_sizes = []\n",
    "        #for block in self.layers.values():\n",
    "        for i in range(len(self.layers)):\n",
    "            conv_block, pooling_layer = self.layers[f'conv_block_{i}']\n",
    "            # Apply the convolutional layer\n",
    "            x = conv_block(x)\n",
    "            # we need to keep track of the input size\n",
    "            # before the pooling, to do the unpooling correctly at decoding time\n",
    "            input_sizes.append(x.size())\n",
    "            # Apply the pooling layer\n",
    "            x, indices = pooling_layer(x)\n",
    "            # We need to keep track of the indices for the unpooling\n",
    "            pooling_indices.append(indices)\n",
    "        x = self.conv1x1(x)\n",
    "\n",
    "        return x, pooling_indices, input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is the mirror image of the encoder. We pass to it the same architecture parameter that was passed to the encoder, and the network is built from right to left in a kind of unintuitive fashion that allows to keep the simmetry and clarity of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],\n",
    "                 out_channels:int=1,) -> None:\n",
    "        super(HorseshoeDecoder, self).__init__()\n",
    "\n",
    "        def transp_conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        out_channels = out_channels     # desired n° of output channels\n",
    "        arch_len = len(architecture)\n",
    "        for i, (n_blocks, in_channels) in enumerate(architecture):\n",
    "            transp_conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                transp_conv_layers.insert(0,transp_conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                out_channels = in_channels\n",
    "            transp_conv_block = nn.Sequential(*transp_conv_layers)\n",
    "            # Since we're building the architecture from right to left, we need to add \n",
    "            # (more precisely, index) the modules in reversed order in the dictionary\n",
    "            self.layers.add_module(f'transp_conv_block_{arch_len-i-1}',nn.ModuleList([nn.MaxUnpool2d(2,2), transp_conv_block])) \n",
    "\n",
    "        self.Tconv1x1 = nn.ConvTranspose2d(256, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, pooling_indices, input_sizes):\n",
    "        x = self.Tconv1x1(x)\n",
    "        #for block in reversed(self.layers.values()):\n",
    "        for i in range(len(self.layers)):\n",
    "            unpooling_layer, transp_conv_block = self.layers[f'transp_conv_block_{i}']\n",
    "            # First the unpooling (which needs the indices used for pooling and the size of the input)...\n",
    "            x = unpooling_layer(x, pooling_indices.pop(), output_size=input_sizes.pop())\n",
    "            # ... then the transposed convolution!\n",
    "            x = transp_conv_block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full `HorseshoeNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've separately written the encoder and the decoder, to create our horseshoe network we just have to put them together, basically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels:int=3,\n",
    "                 out_channels:int=1,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],):\n",
    "        super(HorseshoeNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.encoder = HorseshoeEncoder(architecture=architecture,in_channels=in_channels)\n",
    "\n",
    "        # Transpose conv. layers\n",
    "        self.decoder = HorseshoeDecoder(architecture=architecture,out_channels=out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x, pooling_indices, input_sizes = self.encoder(x)\n",
    "\n",
    "        # Decoding\n",
    "        x = self.decoder(x, pooling_indices, input_sizes)\n",
    "\n",
    "        # Final sigmoid (classification)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 568, 800]) torch.Size([1, 1, 568, 800])\n",
      "torch.float32 torch.uint8\n"
     ]
    }
   ],
   "source": [
    "img, mask = next(iter(train_plain_dataloader))\n",
    "\n",
    "print(img.shape, mask.shape)\n",
    "print(img.dtype, mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HorseshoeNetwork(architecture=[(2,16),(2,32)])\n",
    "model.to(device)\n",
    "\n",
    "out = model(img.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 568, 800]), torch.float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, out.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISPR-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
