{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## ISPR 23/24, third assignment\n",
    "### by Andrea Marino (matr. 561935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import sys\n",
    "#from PIL import Image              # Python Image Library\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(colab := 'google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"drive/MyDrive/ISPR/Compitini ISPR/Terzo Compitino/\"\n",
    "    sys.path.insert(0,BASE_PATH)\n",
    "    #os.chdir(BASE_PATH)\n",
    "    #!pip install -U torch           # update PyTorch\n",
    "    #!pip install -U torchvision     # update torchvision\n",
    "else:\n",
    "    BASE_PATH = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set PyTorch's seed to a fixed value, for reproducibility but also to get the same splits in the datasets we're going to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x751275334a30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data, Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I already used the dataset for the first assignment, I don't need to download it again. I just have to fetch it from the right directory (in a way that deals with both Colab and my local install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Primo Compitino/weizmann_horse_db/horse/',\n",
       " '../Primo Compitino/weizmann_horse_db/mask/')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HORSE_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/horse/\")\n",
    "MASK_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/mask/\")\n",
    "\n",
    "HORSE_PATH, MASK_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create the class for the dataset, as required by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply transformations on our data. The principal transformation worth talking about has to deal with the batch size and the shape of the images. \n",
    "\n",
    "If in our `DataLoader`s the batch size is set to be greater than 1, then the images have to be of the same shape (height * width), otherwise the associated tensors can't be stacked in a single batch tensor. This is unfortunate, we can deal with it in three ways:\n",
    "1. Always online learning algorithm (aka batch_size=1)\n",
    "2. Resize the image\n",
    "3. Add a padding of zeros to the smaller images, to bring them all to the same size as the bigger image in the dataset (and then let the model deal with the extra padding).\n",
    "\n",
    "All three options are interesting for their own reasons, so writing a more general code that allows one to apply each of the ideas is worthwhile.\n",
    "\n",
    "The first option allows to assess the model's performance on images of different sizes, an interesting challenge for sure. The second option is the most promising and most powerful one, as torchvision's tools seems to do a good job at resizing both the images and the mask, upon preliminary checking. The third option is pretty rough, but it might be interesting to see how the model performs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "class HorseDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 image_path=HORSE_PATH,\n",
    "                 mask_path=MASK_PATH,\n",
    "                 transform=Lambda(lambda img: img/255), # normalize pixel values\n",
    "                 target_transform=None,):\n",
    "        self.img_dir = image_path\n",
    "        self.mask_dir = mask_path\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # We know that the answer is 327 but let's make it\n",
    "        # more general and structured\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self._horsePath(idx+1)\n",
    "        img = read_image(img_path)      \n",
    "        mask = read_image(mask_path)  \n",
    "        img = img.float()\n",
    "        mask = mask.float() # convert to float \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        mask[mask>0] = 1.0 # set all non-zero values to 1\n",
    "        return img, mask\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self.__getitem__(i)    \n",
    "    \n",
    "    def _horsePath(self, h:int):\n",
    "        '''\n",
    "        Returns the path to the horse image\n",
    "        whose number (in the filename) is h\n",
    "        '''\n",
    "        number = \"0\"*(2-int(np.log10(h)))+str(h)\n",
    "        imgname = \"horse\" + number + \".png\"\n",
    "        img_path = self.img_dir + imgname\n",
    "        mask_path = self.mask_dir + imgname\n",
    "        return img_path, mask_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the transformations as described, we need the height and width of the biggest and smallest images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest image is horse 65, of size 960x1280\n",
      "The smallest image is horse 16, of size 148x231\n"
     ]
    }
   ],
   "source": [
    "plain_dataset = HorseDataset()\n",
    "\n",
    "MAX, MIN = (0,0), (np.inf,np.inf)\n",
    "biggest, smallest = -1,-1\n",
    "\n",
    "for i, (img, _) in enumerate(plain_dataset,start=1):\n",
    "    img_h, img_w = tuple(img.shape[1:])\n",
    "    if img_h*img_w > MAX[0]*MAX[1]:\n",
    "        MAX = (img_h, img_w)\n",
    "        biggest = i\n",
    "    if img_h*img_w < MIN[0]*MIN[1]:\n",
    "        MIN = (img_h, img_w)\n",
    "        smallest = i\n",
    "\n",
    "print(f\"The biggest image is horse {biggest}, of size {MAX[0]}x{MAX[1]}\")\n",
    "print(f\"The smallest image is horse {smallest}, of size {MIN[0]}x{MIN[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 590, 800]), mask shape: torch.Size([1, 590, 800])\n",
      "Image type: torch.float32, mask type: torch.float32\n",
      "Image values range: 0.0 - 1.0\n",
      "Mask values range: 0.0 - 1.0\n"
     ]
    }
   ],
   "source": [
    "image0, mask0 = plain_dataset[0]\n",
    "print(f\"Image shape: {image0.shape}, mask shape: {mask0.shape}\")\n",
    "print(f\"Image type: {image0.dtype}, mask type: {mask0.dtype}\")\n",
    "print(f\"Image values range: {torch.min(image0).item()} - {torch.max(image0).item()}\")\n",
    "print(f\"Mask values range: {torch.min(mask0).item()} - {torch.max(mask0).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of unchanged images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset following the first option.\n",
    "\n",
    "We can use the `random_split` to create the training set and the test set. \n",
    "\n",
    "20% of the whole dataset is held out as test set. I chose this approach over cross-validation to alleviate the computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create the developement and test datasets\n",
    "train_size = int(0.8 * len(plain_dataset))\n",
    "test_size = len(plain_dataset) - train_size\n",
    "\n",
    "training_plain_dataset, test_plain_dataset = random_split(plain_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the DataLoaders for the training and test data, using batch size of 1 since we applied no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Attenzione:\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_plain_dataloader = DataLoader(training_plain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_plain_dataloader = DataLoader(test_plain_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of resized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the `HorseDataset` class allows us to create a dataset of resized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "\n",
    "#resize_transform = lambda h,w: Lambda(lambda X: Resize((h,w))(X))\n",
    "def resize_transform(height:int, width:int):\n",
    "    return Lambda(lambda X: Resize((height,width))(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can resize to the smallest, or choose any other size\n",
    "h, w = 256, 256\n",
    "\n",
    "dataset_resized = HorseDataset(transform=resize_transform(h,w), \n",
    "                              target_transform=resize_transform(h,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_resized_dataset, test_resized_dataset = random_split(dataset_resized, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_resized_dataloader = DataLoader(training_resized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_resized_dataloader = DataLoader(test_resized_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: n° images is 261, n° of minibatches is: 66\n",
      "Test: n° images is 66, n° of minibatches is: 17\n",
      "Shape of X [N, C, H, W]:  torch.Size([4, 3, 256, 256]) torch.float32\n",
      "Shape of y: [N, C, H, W] torch.Size([4, 1, 256, 256]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training: n° images is {len(train_resized_dataloader.dataset)}, n° of minibatches is: {len(train_resized_dataloader)}\")\n",
    "print(f\"Test: n° images is {len(test_resized_dataloader.dataset)}, n° of minibatches is: {len(test_resized_dataloader)}\")\n",
    "\n",
    "for X, y in train_resized_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape, X.dtype)\n",
    "    print(\"Shape of y: [N, C, H, W]\", y.shape, y.dtype)\n",
    "    break\n",
    "\n",
    "for i, (X, y) in enumerate(train_resized_dataloader):\n",
    "    assert torch.min(y) >= 0 and torch.max(y) <= 1, f\"Error at minibatch {i}: targets should be in [0, 1] for BCELoss. Found min {torch.min(y)} and max {torch.max(y)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(y), torch.max(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of padded images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a dataset of padded images using the `HorseDataset` class.\n",
    "\n",
    "Instead of passing a fixed amount of padding to the transformation, we pass it the target height and width of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Pad\n",
    "\n",
    "#pad_transform = lambda target_h, target_w: Lambda(lambda X: Pad((X.shape[1]-target_h, X.shape[2]-target_w), fill=0)(X))\n",
    "def pad_transform(target_h:int, target_w:int):\n",
    "    return Lambda(lambda X: Pad((X.shape[1]-target_h, X.shape[2]-target_w), fill=0)(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = MAX\n",
    "\n",
    "dataset_padded = HorseDataset(transform=pad_transform(h,w), \n",
    "                              target_transform=pad_transform(h,w))\n",
    "\n",
    "# Check that things are ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded_dataset, test_padded_dataset = random_split(dataset_padded, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_padded_dataloader = DataLoader(training_padded_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_padded_dataloader = DataLoader(test_padded_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is semantic segmentation of the horse from the background. The chosen model for this task is a *U-shaped network* that uses convolution and transposed convolutions. Given the context, such a network could be referred as a horseshoe network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our horseshoe network has two parts: an encoder and a decoder. For the sake of simplicity (and cleanness of code), these are split into two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the `HorseshoeEncoder`. Even though that's somewhat ambiguous and deplorable (the ambiguity is also in the literature), inside a convolutional layer three things will happen:\n",
    "- A given number of convolutional filters will be applied\n",
    "- The (batch of) resulting feature maps will be normalized\n",
    "- An activation function is applied\n",
    "\n",
    "The `conv_layer` nested function returns a convolutional layer as a `Sequential` container. \n",
    "\n",
    "We can specify the structure of our decoder via the `architecture` parameter of the constructor. This parameter is a list of tuple, each tuple being relative to a *block*. \n",
    "\n",
    "In this context, a block is just a sequence of convolutional filters followed by a max pooling. More precisely, each tuple in the list specifies how many convolutional layers we want to stack before applying the pooling (1st component of the tuple), and how many convolutional filters there are in each layer inside the block (2nd component of the tuple).\n",
    "\n",
    "So, in abstract terms, our encoder is just a sequence of blocks. In practice, each block is again a Sequential module, and the encoder's layers are implemented as a `nn.ModuleDict`. A dictionary of modules is used because we need to keep track of the pooling indices of the maximum element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],\n",
    "                 in_channels:int=3,) -> None:\n",
    "        super(HorseshoeEncoder, self).__init__()\n",
    "\n",
    "        def conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        in_channels = in_channels  # initial number of input channels\n",
    "        for i, (n_blocks, out_channels) in enumerate(architecture):\n",
    "            conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                conv_layers.append(conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                in_channels = out_channels\n",
    "            conv_block = nn.Sequential(*conv_layers)\n",
    "            # We add a MaxPooling layer after the prescribed number of conv layers\n",
    "            self.layers.add_module(f'conv_block_{i}',nn.ModuleList([conv_block, nn.MaxPool2d(2,2,return_indices=True)]))\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooling_indices = []\n",
    "        input_sizes = []\n",
    "        #for block in self.layers.values():\n",
    "        for i in range(len(self.layers)):\n",
    "            conv_block, pooling_layer = self.layers[f'conv_block_{i}']\n",
    "            # Apply the convolutional layer\n",
    "            x = conv_block(x)\n",
    "            # we need to keep track of the input size\n",
    "            # before the pooling, to do the unpooling correctly at decoding time\n",
    "            input_sizes.append(x.size())\n",
    "            # Apply the pooling layer\n",
    "            x, indices = pooling_layer(x)\n",
    "            # We need to keep track of the indices for the unpooling\n",
    "            pooling_indices.append(indices)\n",
    "        x = self.conv1x1(x)\n",
    "\n",
    "        return x, pooling_indices, input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is the mirror image of the encoder. We pass to it the same architecture parameter that was passed to the encoder, and the network is built from right to left in a kind of unintuitive fashion that allows to keep the simmetry and clarity of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],\n",
    "                 out_channels:int=1,) -> None:\n",
    "        super(HorseshoeDecoder, self).__init__()\n",
    "\n",
    "        def transp_conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        out_channels = out_channels     # desired n° of output channels\n",
    "        arch_len = len(architecture)\n",
    "        for i, (n_blocks, in_channels) in enumerate(architecture):\n",
    "            transp_conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                transp_conv_layers.insert(0,transp_conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                out_channels = in_channels\n",
    "            transp_conv_block = nn.Sequential(*transp_conv_layers)\n",
    "            # Since we're building the architecture from right to left, we need to add \n",
    "            # (more precisely, index) the modules in reversed order in the dictionary\n",
    "            self.layers.add_module(f'transp_conv_block_{arch_len-i-1}',nn.ModuleList([nn.MaxUnpool2d(2,2), transp_conv_block])) \n",
    "\n",
    "        self.Tconv1x1 = nn.ConvTranspose2d(256, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, pooling_indices, input_sizes):\n",
    "        x = self.Tconv1x1(x)\n",
    "        #for block in reversed(self.layers.values()):\n",
    "        for i in range(len(self.layers)):\n",
    "            unpooling_layer, transp_conv_block = self.layers[f'transp_conv_block_{i}']\n",
    "            # First the unpooling (which needs the indices used for pooling and the size of the input)...\n",
    "            x = unpooling_layer(x, pooling_indices.pop(), output_size=input_sizes.pop())\n",
    "            # ... then the transposed convolution!\n",
    "            x = transp_conv_block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full `HorseshoeNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've separately written the encoder and the decoder, to create our horseshoe network we just have to put them together, basically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels:int=3,\n",
    "                 out_channels:int=1,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],):\n",
    "        super(HorseshoeNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.encoder = HorseshoeEncoder(architecture=architecture,in_channels=in_channels)\n",
    "\n",
    "        # Transpose conv. layers\n",
    "        self.decoder = HorseshoeDecoder(architecture=architecture,out_channels=out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x, pooling_indices, input_sizes = self.encoder(x)\n",
    "\n",
    "        # Decoding\n",
    "        x = self.decoder(x, pooling_indices, input_sizes)\n",
    "\n",
    "        # Final sigmoid (classification)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HorseshoeNetwork(\n",
       "  (encoder): HorseshoeEncoder(\n",
       "    (layers): ModuleDict(\n",
       "      (conv_block_0): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv_block_1): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv_block_2): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv_block_3): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv_block_4): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (conv1x1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (decoder): HorseshoeDecoder(\n",
       "    (layers): ModuleDict(\n",
       "      (transp_conv_block_4): ModuleList(\n",
       "        (0): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (transp_conv_block_3): ModuleList(\n",
       "        (0): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (transp_conv_block_2): ModuleList(\n",
       "        (0): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (transp_conv_block_1): ModuleList(\n",
       "        (0): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (transp_conv_block_0): ModuleList(\n",
       "        (0): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Tconv1x1): ConvTranspose2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our HorseshoeNetwork model\n",
    "model = HorseshoeNetwork()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the transposed convolutional blocks are listed in reversed order in the decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training(-test) cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model and the dataset, it's time to write the training cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer) -> None:\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        ## Debugging: Check the data range\n",
    "        assert torch.min(y) >= 0 and torch.max(y) <= 1, \"Targets should be in [0, 1] for BCELoss\"\n",
    "\n",
    "        ## Forward pass:\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "\n",
    "        ## Debugging: Check the prediction range\n",
    "        assert torch.min(pred) >= 0 and torch.max(pred) <= 1, \"Predictions should be in [0, 1] after sigmoid\"\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        ## Backpropagation:\n",
    "        # Zero out the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the gradient (backward step)\n",
    "        loss.backward()\n",
    "        # apply weight update\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To review, obviously\n",
    "def evaluate(dataloader, model, loss_fn) -> float:\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # compute accuracy\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    #print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here to change the preprocessing strategy\n",
    "train_dataloader = train_resized_dataloader\n",
    "test_dataloader = test_resized_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/andrea/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_loss = evaluate(train_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uffa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1, 3, 595, 504]), y shape: torch.Size([1, 1, 595, 504])\n",
      "X dtype: torch.float32, y dtype: torch.float32\n",
      "Prediction shape: torch.Size([1, 1, 595, 504])\n",
      "Prediction dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "dataloader_cp = DataLoader(training_plain_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for i, (X, y) in enumerate(dataloader_cp):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    print(f\"X dtype: {X.dtype}, y dtype: {y.dtype}\")\n",
    "    pred = model(X)\n",
    "    print(f\"Prediction shape: {pred.shape}\")\n",
    "    print(f\"Prediction dtype: {pred.dtype}\")\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISPR-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
