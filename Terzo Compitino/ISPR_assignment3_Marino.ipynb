{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## ISPR 23/24, third midterm\n",
    "### by Andrea Marino (matr. 561935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import sys\n",
    "#from PIL import Image              # Python Image Library\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(colab := 'google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"drive/MyDrive/ISPR/Compitini ISPR/Terzo Compitino/\"\n",
    "    sys.path.insert(0,BASE_PATH)\n",
    "    #os.chdir(BASE_PATH)\n",
    "    #!pip install -U torch           # update PyTorch\n",
    "    #!pip install -U torchvision     # update torchvision\n",
    "else:\n",
    "    BASE_PATH = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set PyTorch's seed to a fixed value, for reproducibility but also to get the same splits in the datasets we're going to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f752ff48a50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data, Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I already used the dataset for the first assignment, I don't need to download it again. I just have to fetch it from the right directory (in a way that deals with both Colab and my local install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Primo Compitino/weizmann_horse_db/horse/',\n",
       " '../Primo Compitino/weizmann_horse_db/mask/')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HORSE_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/horse/\")\n",
    "MASK_PATH = os.path.join(BASE_PATH,\"../Primo Compitino/weizmann_horse_db/mask/\")\n",
    "\n",
    "HORSE_PATH, MASK_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create the class for the dataset, as required by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply transformations on our data. The principal transformation worth talking about has to deal with the batch size and the shape of the images. \n",
    "\n",
    "If in our `DataLoader`s the batch size is set to be greater than 1, then the images have to be of the same shape (height * width), otherwise the associated tensors can't be stacked in a single batch tensor. This is unfortunate, we can deal with it in three ways:\n",
    "1. Always online learning algorithm (aka batch_size=1)\n",
    "2. Resize the image\n",
    "3. Add a padding of zeros to the smaller images, to bring them all to the same size as the bigger image in the dataset (and then let the model deal with the extra padding).\n",
    "\n",
    "All three options are interesting for their own reasons, so writing a more general code that allows one to apply each of the ideas is worthwhile.\n",
    "\n",
    "The first option allows to assess the model's performance on images of different sizes, an interesting challenge for sure. The second option is the most promising and most powerful one, as torchvision's tools seems to do a good job at resizing both the images and the mask, upon preliminary checking. The third option is pretty rough, but it might be interesting to see how the model performs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "class HorseDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 image_path=HORSE_PATH,\n",
    "                 mask_path=MASK_PATH,\n",
    "                 transform=None, # normalize pixel values\n",
    "                 target_transform=None,):\n",
    "        self.img_dir = image_path\n",
    "        self.mask_dir = mask_path\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # We know that the answer is 327 but let's make it\n",
    "        # more general and structured\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self._horsePath(idx+1)\n",
    "        img = read_image(img_path)      \n",
    "        mask = read_image(mask_path)  \n",
    "        img, mask = img.float(), mask.float() # convert to float \n",
    "        # transforms given as input\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        # standard transforms\n",
    "        img = img/torch.max(img)    # normalize pixel values\n",
    "        mask[mask>0] = 1.0          # set all non-zero values to 1\n",
    "        return img, mask\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self.__getitem__(i)    \n",
    "    \n",
    "    def _horsePath(self, h:int):\n",
    "        '''\n",
    "        Returns the path to the horse image\n",
    "        whose number (in the filename) is h\n",
    "        '''\n",
    "        number = \"0\"*(2-int(np.log10(h)))+str(h)\n",
    "        imgname = \"horse\" + number + \".png\"\n",
    "        img_path = self.img_dir + imgname\n",
    "        mask_path = self.mask_dir + imgname\n",
    "        return img_path, mask_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the transformations as described, we may need the height and width of the biggest and smallest images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest image is horse 65, of size 960x1280\n",
      "The smallest image is horse 16, of size 148x231\n"
     ]
    }
   ],
   "source": [
    "plain_dataset = HorseDataset()\n",
    "\n",
    "MAX, MIN = (0,0), (np.inf,np.inf)\n",
    "biggest, smallest = -1,-1\n",
    "\n",
    "for i, (img, _) in enumerate(plain_dataset,start=1):\n",
    "    img_h, img_w = tuple(img.shape[1:])\n",
    "    if img_h*img_w > MAX[0]*MAX[1]:\n",
    "        MAX = (img_h, img_w)\n",
    "        biggest = i\n",
    "    if img_h*img_w < MIN[0]*MIN[1]:\n",
    "        MIN = (img_h, img_w)\n",
    "        smallest = i\n",
    "\n",
    "print(f\"The biggest image is horse {biggest}, of size {MAX[0]}x{MAX[1]}\")\n",
    "print(f\"The smallest image is horse {smallest}, of size {MIN[0]}x{MIN[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 590, 800]), mask shape: torch.Size([1, 590, 800])\n",
      "Image type: torch.float32, mask type: torch.float32\n",
      "Image values range: 0.0 - 1.0\n",
      "Mask values range: 0.0 - 1.0\n"
     ]
    }
   ],
   "source": [
    "image0, mask0 = plain_dataset[0]\n",
    "print(f\"Image shape: {image0.shape}, mask shape: {mask0.shape}\")\n",
    "print(f\"Image type: {image0.dtype}, mask type: {mask0.dtype}\")\n",
    "print(f\"Image values range: {torch.min(image0).item()} - {torch.max(image0).item()}\")\n",
    "print(f\"Mask values range: {torch.min(mask0).item()} - {torch.max(mask0).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of unchanged images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset following the first option.\n",
    "\n",
    "We can use the `random_split` to create the training set and the test set. \n",
    "\n",
    "20% of the whole dataset is held out as test set. I chose this approach over cross-validation to alleviate the computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create the developement and test datasets\n",
    "train_size = int(0.8 * len(plain_dataset))\n",
    "test_size = len(plain_dataset) - train_size\n",
    "\n",
    "training_plain_dataset, test_plain_dataset = random_split(plain_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the DataLoaders for the training and test data, using batch size of 1 since we applied no transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Attenzione:\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_plain_dataloader = DataLoader(training_plain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_plain_dataloader = DataLoader(test_plain_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: n° images is 261, n° of minibatches is: 261\n",
      "Test: n° images is 66, n° of minibatches is: 66\n",
      "Shape of X [N, C, H, W]:  torch.Size([1, 3, 207, 335]) torch.float32\n",
      "Shape of y: [N, C, H, W] torch.Size([1, 1, 207, 335]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training: n° images is {len(train_plain_dataloader.dataset)}, n° of minibatches is: {len(train_plain_dataloader)}\")\n",
    "print(f\"Test: n° images is {len(test_plain_dataloader.dataset)}, n° of minibatches is: {len(test_plain_dataloader)}\")\n",
    "\n",
    "for X, y in train_plain_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape, X.dtype)\n",
    "    print(\"Shape of y: [N, C, H, W]\", y.shape, y.dtype)\n",
    "    break\n",
    "\n",
    "for i, (X, y) in enumerate(train_plain_dataloader):\n",
    "    assert torch.min(y) >= 0 and torch.max(y) <= 1, f\"Error at minibatch {i}: targets should be in [0, 1] for BCELoss. Found min {torch.min(y)} and max {torch.max(y)}\"\n",
    "    assert torch.min(X) >= 0 and torch.max(X) <= 1, f\"Error at minibatch {i}: inputs should be in [0, 1]. Found min {torch.min(X)} and max {torch.max(X)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of resized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the `HorseDataset` class allows us to create a dataset of resized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "\n",
    "#resize_transform = lambda h,w: Lambda(lambda X: Resize((h,w))(X))\n",
    "def resize_transform(height:int, width:int):\n",
    "    return Lambda(lambda X: Resize((height,width))(X))\n",
    "\n",
    "# We can resize to the smallest, or choose any other size\n",
    "h, w = 360, 480\n",
    "\n",
    "dataset_resized = HorseDataset(transform=resize_transform(h,w), \n",
    "                              target_transform=resize_transform(h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 360, 480]), mask shape: torch.Size([1, 360, 480])\n",
      "Image type: torch.float32, mask type: torch.float32\n",
      "Image values range: 0.0 - 1.0\n",
      "Mask values range: 0.0 - 1.0\n"
     ]
    }
   ],
   "source": [
    "image0, mask0 = dataset_resized[0]\n",
    "print(f\"Image shape: {image0.shape}, mask shape: {mask0.shape}\")\n",
    "print(f\"Image type: {image0.dtype}, mask type: {mask0.dtype}\")\n",
    "print(f\"Image values range: {torch.min(image0).item()} - {torch.max(image0).item()}\")\n",
    "print(f\"Mask values range: {torch.min(mask0).item()} - {torch.max(mask0).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_resized_dataset, test_resized_dataset = random_split(dataset_resized, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_resized_dataloader = DataLoader(training_resized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_resized_dataloader = DataLoader(test_resized_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: n° images is 261, n° of minibatches is: 66\n",
      "Test: n° images is 66, n° of minibatches is: 17\n",
      "Shape of X [N, C, H, W]:  torch.Size([4, 3, 360, 480]) torch.float32\n",
      "Shape of y: [N, C, H, W] torch.Size([4, 1, 360, 480]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training: n° images is {len(train_resized_dataloader.dataset)}, n° of minibatches is: {len(train_resized_dataloader)}\")\n",
    "print(f\"Test: n° images is {len(test_resized_dataloader.dataset)}, n° of minibatches is: {len(test_resized_dataloader)}\")\n",
    "\n",
    "for X, y in train_resized_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape, X.dtype)\n",
    "    print(\"Shape of y: [N, C, H, W]\", y.shape, y.dtype)\n",
    "    break\n",
    "\n",
    "for i, (X, y) in enumerate(train_resized_dataloader):\n",
    "    assert torch.min(y) >= 0 and torch.max(y) <= 1, f\"Error at minibatch {i}: targets should be in [0, 1] for BCELoss. Found min {torch.min(y)} and max {torch.max(y)}\"\n",
    "    assert torch.min(X) >= 0 and torch.max(X) <= 1, f\"Error at minibatch {i}: inputs should be in [0, 1]. Found min {torch.min(X)} and max {torch.max(X)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horse dataset of padded images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a dataset of padded images using the `HorseDataset` class.\n",
    "\n",
    "Instead of passing a fixed amount of padding to the transformation, we pass it the target height and width of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Pad\n",
    "\n",
    "#pad_transform = lambda target_h, target_w: Lambda(lambda X: Pad((X.shape[1]-target_h, X.shape[2]-target_w), fill=0)(X))\n",
    "def pad_transform(target_h:int, target_w:int):\n",
    "    return Lambda(lambda X: Pad((target_h-X.shape[1], target_w-X.shape[2]), fill=0)(X))\n",
    "\n",
    "h, w = MAX\n",
    "\n",
    "dataset_padded = HorseDataset(transform=pad_transform(h,w), \n",
    "                              target_transform=pad_transform(h,w))\n",
    "\n",
    "# Check that things are ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded_dataset, test_padded_dataset = random_split(dataset_padded, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_padded_dataloader = DataLoader(training_padded_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_padded_dataloader = DataLoader(test_padded_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasciamo perdere per ora va\n",
    "\n",
    "#print(f\"Training: n° images is {len(train_padded_dataloader.dataset)}, n° of minibatches is: {len(train_padded_dataloader)}\")\n",
    "#print(f\"Test: n° images is {len(test_padded_dataloader.dataset)}, n° of minibatches is: {len(test_padded_dataloader)}\")\n",
    "#\n",
    "#for X, y in train_padded_dataloader:\n",
    "#    print(\"Shape of X [N, C, H, W]: \", X.shape, X.dtype)\n",
    "#    print(\"Shape of y: [N, C, H, W]\", y.shape, y.dtype)\n",
    "#    break\n",
    "#\n",
    "#for i, (X, y) in enumerate(train_padded_dataloader):\n",
    "#    assert torch.min(y) >= 0 and torch.max(y) <= 1, f\"Error at minibatch {i}: targets should be in [0, 1] for BCELoss. Found min {torch.min(y)} and max {torch.max(y)}\"\n",
    "#    assert torch.min(X) >= 0 and torch.max(X) <= 1, f\"Error at minibatch {i}: inputs should be in [0, 1]. Found min {torch.min(X)} and max {torch.max(X)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is semantic segmentation of the horse from the background. The chosen model for this task is a *U-shaped network* that uses convolution and transposed convolutions. Given the context, such a network could be referred as a horseshoe network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our horseshoe network has two parts: an encoder and a decoder. For the sake of simplicity (and cleanness of code), these are split into two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture I chose is heavily inspired by [SegNet](https://arxiv.org/abs/1511.00561), because it's a network that was designed with the precise purpose of image segmentation, in an (parameter-wise) efficient way.\n",
    "\n",
    "The architecture I propose is modular enough to incorporate some changes, though. For example, a feed-forward layer between the encoder and the decoder can be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the `HorseshoeEncoder`. Even though that's somewhat ambiguous and deplorable (the ambiguity is also in the literature), inside a convolutional layer three things will happen:\n",
    "- A given number of convolutional filters will be applied\n",
    "- The (batch of) resulting feature maps will be normalized\n",
    "- An activation function is applied\n",
    "\n",
    "The `conv_layer` nested function returns a convolutional layer as a `Sequential` container. \n",
    "\n",
    "We can specify the structure of our decoder via the `architecture` parameter of the constructor. This parameter is a list of tuple, each tuple being relative to a *block*. \n",
    "\n",
    "In this context, a block is just a sequence of convolutional filters followed by a max pooling. More precisely, each tuple in the list specifies how many convolutional layers we want to stack before applying the pooling (1st component of the tuple), and how many convolutional filters there are in each layer inside the block (2nd component of the tuple).\n",
    "\n",
    "So, in abstract terms, our encoder is just a sequence of blocks. In practice, each block is again a Sequential module, and the encoder's layers are implemented as a `nn.ModuleDict`. A dictionary of modules is used because we need to keep track of the pooling indices of the maximum element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],\n",
    "                 in_channels:int=3,) -> None:\n",
    "        super(HorseshoeEncoder, self).__init__()\n",
    "\n",
    "        def conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        in_channels = in_channels  # initial number of input channels\n",
    "        for i, (n_blocks, out_channels) in enumerate(architecture):\n",
    "            conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                conv_layers.append(conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                in_channels = out_channels\n",
    "            conv_block = nn.Sequential(*conv_layers)\n",
    "            # We add a MaxPooling layer after the prescribed number of conv layers\n",
    "            self.layers.add_module(f'conv_block_{i}',nn.ModuleList([conv_block, nn.MaxPool2d(2,2,return_indices=True)]))\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooling_indices = []\n",
    "        input_sizes = []\n",
    "        #for block in self.layers.values():\n",
    "        for i in range(len(self.layers)):\n",
    "            conv_block, pooling_layer = self.layers[f'conv_block_{i}']\n",
    "            # Apply the convolutional layer\n",
    "            x = conv_block(x)\n",
    "            # we need to keep track of the input size\n",
    "            # before the pooling, to do the unpooling correctly at decoding time\n",
    "            input_sizes.append(x.size())\n",
    "            # Apply the pooling layer\n",
    "            x, indices = pooling_layer(x)\n",
    "            # We need to keep track of the indices for the unpooling\n",
    "            pooling_indices.append(indices)\n",
    "        x = self.conv1x1(x)\n",
    "\n",
    "        return x, pooling_indices, input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is the mirror image of the encoder. We pass to it the same architecture parameter that was passed to the encoder, and the network is built from right to left in a kind of unintuitive fashion that allows to keep the simmetry and clarity of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],\n",
    "                 out_channels:int=2,) -> None:\n",
    "        super(HorseshoeDecoder, self).__init__()\n",
    "\n",
    "        def transp_conv_layer(in_channels, out_channels, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        out_channels = out_channels     # desired n° of output channels\n",
    "        arch_len = len(architecture)\n",
    "        for i, (n_blocks, in_channels) in enumerate(architecture):\n",
    "            transp_conv_layers = []\n",
    "            for _ in range(n_blocks):\n",
    "                transp_conv_layers.insert(0,transp_conv_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=0))\n",
    "                out_channels = in_channels\n",
    "            transp_conv_block = nn.Sequential(*transp_conv_layers)\n",
    "            # Since we're building the architecture from right to left, we need to add \n",
    "            # (more precisely, index) the modules in reversed order in the dictionary\n",
    "            self.layers.add_module(f'transp_conv_block_{arch_len-i-1}',nn.ModuleList([nn.MaxUnpool2d(2,2), transp_conv_block])) \n",
    "\n",
    "        self.Tconv1x1 = nn.ConvTranspose2d(256, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, pooling_indices, input_sizes):\n",
    "        x = self.Tconv1x1(x)\n",
    "        #for block in reversed(self.layers.values()):\n",
    "        for i in range(len(self.layers)):\n",
    "            unpooling_layer, transp_conv_block = self.layers[f'transp_conv_block_{i}']\n",
    "            # First the unpooling (which needs the indices used for pooling and the size of the input)...\n",
    "            x = unpooling_layer(x, pooling_indices.pop(), output_size=input_sizes.pop())\n",
    "            # ... then the transposed convolution!\n",
    "            x = transp_conv_block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full `HorseshoeNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've separately written the encoder and the decoder, to create our horseshoe network we just have to put them together, basically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseshoeNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels:int=3,\n",
    "                 out_channels:int=2,\n",
    "                 architecture:List[Tuple]=[(2,16),(2,32),(3,64),(3,128),(3,128)],):\n",
    "        super(HorseshoeNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.encoder = HorseshoeEncoder(architecture=architecture,in_channels=in_channels)\n",
    "\n",
    "        # Transpose conv. layers\n",
    "        self.decoder = HorseshoeDecoder(architecture=architecture,out_channels=out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x, pooling_indices, input_sizes = self.encoder(x)\n",
    "\n",
    "        # Decoding\n",
    "        x = self.decoder(x, pooling_indices, input_sizes)\n",
    "\n",
    "        # Final softmax (classification)\n",
    "        #print(f\"Range before sigmoid: {torch.min(x).item()} - {torch.max(x).item()}\")\n",
    "        x = F.softmax(x, dim=1)\n",
    "        #print(f\"Range after sigmoid: {torch.min(x).item()} - {torch.max(x).item()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the transposed convolutional blocks are listed in reversed order in the decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model and the dataset, it's time to write the training cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the typical PyTorch's paradygm, let's define a function that performs one cycle of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer) -> None:\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        assert torch.unique(y).tolist() == [0., 1.], f\"Error: targets should be in [0, 1] for BCELoss. Instead got: {torch.unique(y).tolist()}, type: {y.dtype}\"\n",
    "        assert X.shape[1] == 3, f\"Error: input images should have 3 channels. Instead got: {X.shape[1]}\"\n",
    "        assert torch.max(X) <= 1 and torch.min(X) >= 0, f\"Error: input images should be normalized in [0, 1]. Instead got: min {torch.min(X).item()} and max {torch.max(X).item()}\"\n",
    "\n",
    "        ## Forward pass:\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        ## Backpropagation:\n",
    "        # Zero out the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the gradient (backward step)\n",
    "        loss.backward()\n",
    "        # apply weight update\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to evaluate the model (aka to test it). That's what the next function does. \n",
    "\n",
    "This function can return multiple metrics, but for this assignment only the accuracy will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(y_true, \n",
    "                  y_pred, \n",
    "                  score_fn:str='accuracy') -> float:\n",
    "    # debugging\n",
    "    #print(f\"Unique values: {torch.unique(y_pred)}\")\n",
    "    \n",
    "    # turn the probability into classes\n",
    "    y_pred_bin = (y_pred >= 0.5).float()\n",
    "    \n",
    "    # Legacy code:\n",
    "    #accuracy = (y_true == y_pred_bin).sum().item() / y_true.numel()\n",
    "\n",
    "    TPs = (y_true * y_pred_bin).sum().item()\n",
    "    TNs = ((1 - y_true) * (1 - y_pred_bin)).sum().item()\n",
    "    FPs = ((1 - y_true) * y_pred_bin).sum().item()\n",
    "    FNs = (y_true * (1 - y_pred_bin)).sum().item()\n",
    "\n",
    "    # Since I compute the average, tuurns out the formulas are the ones\n",
    "    # below\n",
    "    score = {}\n",
    "    score['accuracy'] = (TPs + TNs) / (TPs + TNs + FPs + FNs)\n",
    "    score['precision'] = TPs / (TPs + FPs) if TPs + FPs != 0 else 0\n",
    "    score['recall'] = TPs / (TPs + FNs) if TPs + FNs != 0 else 0\n",
    "\n",
    "    if score_fn == 'whole':\n",
    "        return score\n",
    "    else:\n",
    "        return score[score_fn.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To review, obviously\n",
    "def evaluate(dataloader, model, loss_fn) -> float:\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    avg_accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            #print(f\"y min: {torch.min(y)}, y max: {torch.max(y)}\")\n",
    "            \n",
    "            pred = model(X)\n",
    "            #print(f\"pred min: {torch.min(pred)}, pred max: {torch.max(pred)}\")\n",
    "\n",
    "            # compute the loss, and accumulate it\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # compute accuracy, and accumulate it\n",
    "            avg_accuracy += compute_score(y_true=y, y_pred=pred, score_fn='accuracy')\n",
    "    # Average over the number of batches\n",
    "    avg_accuracy /= num_batches\n",
    "    test_loss /= num_batches\n",
    "    return test_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here to change the preprocessing strategy\n",
    "train_dataloader = train_plain_dataloader\n",
    "test_dataloader = test_plain_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model, define the loss and the learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our HorseshoeNetwork model\n",
    "model = HorseshoeNetwork(architecture=[(2,32),(2,64)])\n",
    "model.to(device)\n",
    "\n",
    "# Define loss and learning algorithm\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1, 1, 507, 800])) that is different to the input size (torch.Size([1, 2, 507, 800])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Compute the loss and accuracy\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m evaluate(train_dataloader, model, loss_fn)\n",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m## Backpropagation:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Zero out the gradient\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISPR-env/lib/python3.12/site-packages/torch/nn/functional.py:3145\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3143\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3148\u001b[0m     )\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([1, 1, 507, 800])) that is different to the input size (torch.Size([1, 2, 507, 800])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = [], []\n",
    "\n",
    "EPOCHS = 5\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    # Compute the loss and accuracy\n",
    "    loss, acc = evaluate(train_dataloader, model, loss_fn)\n",
    "\n",
    "    # print\n",
    "    print(f\"Train Error: \\n \\tAvg loss: {loss:>8f}, avg accuracy: {100*acc:>2.4f}% \\n\")\n",
    "    # Store loss and accuracy for the current epoch\n",
    "    train_loss.append(loss)\n",
    "    train_accuracy.append(acc)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].plot(train_loss, label='Loss')\n",
    "ax[0].set_title('Training Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_accuracy, label='Accuracy')\n",
    "ax[1].set_title('Training Accuracy')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try on a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = test_plain_dataset[0]\n",
    "img, mask = img.to(device), mask.to(device)\n",
    "pred = model(img[None, ...])    # add a batch dimension\n",
    "\n",
    "\n",
    "pred = pred.squeeze()           # remove the batch and channel dimensions (assuming 1 output channel for the prediction)\n",
    "pred_bin = (pred >= 0.5).float() # binarize the prediction\n",
    "#img /= img.max()                # Normalize the image (for visualization purposes)\n",
    "\n",
    "print(pred.shape, mask.shape, img.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "ax[0].imshow(img.cpu().permute(1, 2, 0))\n",
    "ax[0].set_title('Image', fontsize=16, fontweight='bold')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(mask.cpu().squeeze(), cmap='gray')\n",
    "ax[1].set_title('Mask', fontsize=16, fontweight='bold')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(pred.cpu().detach(), cmap='gray')\n",
    "ax[2].set_title('Prediction', fontsize=16, fontweight='bold')\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.unique(pred.detach(), return_counts=True))\n",
    "print(torch.unique(pred_bin, return_counts=True))\n",
    "print(torch.unique(mask, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altro (discarica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_range = [1e-3, 1e-4, 1e-5],\n",
    "EPOCHS = 10\n",
    "\n",
    "for lr in lr_range:\n",
    "    model = HorseshoeNetwork()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for t in range(EPOCHS):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss, avg_accuracy = evaluate(test_dataloader, model, loss_fn)\n",
    "        print(f\"Test Error: \\n Accuracy: {avg_accuracy}, Avg loss: {test_loss}\")\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to try with Optuna. Because I don't have time for a full grid search..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Define the hyperparameters search space\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n",
    "    architecture = [(trial.suggest_int(f\"n_blocks_{i}\", 1, 5), trial.suggest_int(f\"out_channels_{i}\", 16, 256)) for i in range(5)]\n",
    "\n",
    "    # Define the model, with the suggested architecture\n",
    "    model = HorseshoeNetwork(architecture=architecture)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(10):\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    # Evaluation\n",
    "    test_loss, avg_accuracy = evaluate(test_dataloader, model, loss_fn)\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISPR-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
